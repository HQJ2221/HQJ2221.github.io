<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/icons8-fox-32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/icons8-fox-16.png">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif:300,300italic,400,400italic,700,700italic%7CJetBrains+Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/black/pace-theme-center-atom.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"hqj2221.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":280,"display":"post","padding":18,"offset":20},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="The most basic AI knowledge, including some ML algorithm and concept of Agents.">
<meta property="og:type" content="article">
<meta property="og:title" content="CS303 人工智能">
<meta property="og:url" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/index.html">
<meta property="og:site_name" content="Ho Kai Kwan&#39;s Personal Pages">
<meta property="og:description" content="The most basic AI knowledge, including some ML algorithm and concept of Agents.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai101.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai102.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai201.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai202.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai203.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai204.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai301.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai302.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai303.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai304.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai305.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai401.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai501.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai502.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai503.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai504.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai505.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai701.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai702.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai801.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai802.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai803.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai804.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai901.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1001.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1002.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1003.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1004.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1005.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1006.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1007.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1008.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1009.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1010.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1011.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1012.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1013.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1014.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1015.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1016.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1017.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1018.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1019.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1020.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1021.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1022.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1023.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1024.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1025.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1101.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1201.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1202.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1203.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1204.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1205.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1206.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1207.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1208.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1209.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1210.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1301.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1302.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1303.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1304.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1305.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1306.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1307.png">
<meta property="og:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai1308.png">
<meta property="article:published_time" content="2025-01-01T14:05:22.000Z">
<meta property="article:modified_time" content="2025-01-07T06:51:02.654Z">
<meta property="article:author" content="Bionic l&#39;Hôpital">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="CSE Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/ai101.png">


<link rel="canonical" href="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/","path":"2025/01/01/Artificial-Intelligence/","title":"CS303 人工智能"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>CS303 人工智能 | Ho Kai Kwan's Personal Pages</title>
  







<script>
  window.addEventListener('DOMContentLoaded', () => {
    'use strict';
    
      if (NexT.utils.hasMobileUA()) return;
    
    let time, hidden, visible, title = document.title;
    let favicon = document.querySelector('link[rel="icon"]');
    
      hidden = 'ヽ（≧□≦）ノ,Waiting you back...';
      visible = 'o(*￣▽￣*)ブ,Welcome back!';
    
    let random = t => t[Math.floor(Math.random() * t.length)];
    const change = () => {
      if (document.hidden) {
        favicon.setAttribute('href', '/images/favicon-32x32-next.png');
        
          document.title = hidden;
        
        clearTimeout(time);
      } else {
        favicon.setAttribute('href', '/images/favicon-32x32-next.png');
        
          document.title = visible;
        
        time = setTimeout(() => {
          document.title = title;
        }, 200);
      }
    }
    document.addEventListener('visibilitychange', change, false);
  });
</script>

  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Ho Kai Kwan's Personal Pages</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">A personal site by Ho Kai Kwan,</br>junior student from South University of Science and Technology.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">13</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">9</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fas fa-list fa-fw"></i>Categories<span class="badge">7</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Contents"><span class="nav-text">Contents</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-1-AI-as-Search"><span class="nav-text">Lecture 1. AI as Search</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Outline"><span class="nav-text">Outline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Search-in-a-Tree"><span class="nav-text">Search in a Tree ?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Un-informed-Search-Methods"><span class="nav-text">Un-informed Search Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BFS"><span class="nav-text">BFS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#UCS-Uniform-Cost-Search"><span class="nav-text">UCS (Uniform-Cost Search)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DFS"><span class="nav-text">DFS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DLS-Depth-Limit-Search"><span class="nav-text">DLS (Depth-Limit Search)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#IDS-Iterative-Deepening-Search"><span class="nav-text">IDS (Iterative Deepening Search)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bi-directional-Search"><span class="nav-text">Bi-directional Search</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Heuristic-informed-Search"><span class="nav-text">Heuristic (informed) Search</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Greedy-Best-first-Search"><span class="nav-text">Greedy Best-first Search</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Search"><span class="nav-text">A* Search</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-2-Beyond-Classical-Search"><span class="nav-text">Lecture 2. Beyond Classical Search</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Outline-1"><span class="nav-text">Outline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#More-representations"><span class="nav-text">More representations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Direct-Search-in-the-Solution-Space"><span class="nav-text">Direct Search in the Solution Space</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#General-Search-Frameworks"><span class="nav-text">General Search Frameworks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Typical-Search-Operators"><span class="nav-text">Typical Search Operators</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Greedy-Local-Search-Framework"><span class="nav-text">Greedy Local Search Framework</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SA-Tabu-and-Bayesian-Optimizations"><span class="nav-text">SA, Tabu and Bayesian Optimizations</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Simulated-Annealing"><span class="nav-text">Simulated Annealing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tabu"><span class="nav-text">Tabu</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bayesian-Optimizations"><span class="nav-text">Bayesian Optimizations</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Population-based-Search"><span class="nav-text">Population-based Search</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-text">Summary</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-3-Problem-Specific-Search"><span class="nav-text">Lecture 3. Problem-Specific Search</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Outline-2"><span class="nav-text">Outline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Make-Search-Algorithms-Less-General"><span class="nav-text">Make Search Algorithms Less General</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-based-Methods-for-Numerical-Optimization"><span class="nav-text">Gradient-based Methods for Numerical Optimization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Quadratic-Programming-Problems"><span class="nav-text">Quadratic Programming Problems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Constraint-Satisfaction-Problems-CSP"><span class="nav-text">Constraint Satisfaction Problems (CSP)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Characteristics-of-CSPs"><span class="nav-text">Characteristics of CSPs</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Real-world-CSPs"><span class="nav-text">Real-world CSPs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Commutativity"><span class="nav-text">Commutativity</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Constraint-Graph"><span class="nav-text">Constraint Graph</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inference"><span class="nav-text">Inference</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Backtracking-Search-for-CSP"><span class="nav-text">Backtracking Search for CSP</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adversarial-%E5%AF%B9%E6%8A%97%E6%80%A7-Search"><span class="nav-text">Adversarial(对抗性) Search</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Alpha-Beta-Pruning"><span class="nav-text">Alpha-Beta Pruning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary-on-Search"><span class="nav-text">Summary on Search</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-4-Principles-of-Machine-Learning"><span class="nav-text">Lecture 4. Principles of Machine Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Outline-3"><span class="nav-text">Outline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-is-Learning"><span class="nav-text">What is Learning?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Two-simple-methods"><span class="nav-text">Two simple methods</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Key-Questions-for-Machine-Learning"><span class="nav-text">Key Questions for Machine Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Learning-Paradigms-and-Principles"><span class="nav-text">Learning Paradigms and Principles</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-5-Supervised-Learning"><span class="nav-text">Lecture 5. Supervised Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Outline-4"><span class="nav-text">Outline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-Discriminant-Analysis"><span class="nav-text">Linear Discriminant Analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Support-Vector-Machine"><span class="nav-text">Support Vector Machine</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Artificial-Neural-Networks"><span class="nav-text">Artificial Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-NN"><span class="nav-text">Training NN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Some-issues"><span class="nav-text">Some issues</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Decision-Tree"><span class="nav-text">Decision Tree</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-6-Performance-Evaluation-for-Machine-Learning"><span class="nav-text">Lecture 6. Performance Evaluation for Machine Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Outline-5"><span class="nav-text">Outline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Brief-view"><span class="nav-text">Brief view</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Performance-Metrics"><span class="nav-text">Performance Metrics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Estimating-the-Generalization"><span class="nav-text">Estimating the Generalization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-7-Unsupervised-Learning"><span class="nav-text">Lecture 7. Unsupervised Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Outline-6"><span class="nav-text">Outline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-Unsupervised"><span class="nav-text">Why Unsupervised ?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Clustering"><span class="nav-text">Clustering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K-Means"><span class="nav-text">K-Means</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Algorithm"><span class="nav-text">Algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Application-Clustering-for-Graph-Data"><span class="nav-text">Application: Clustering for Graph Data</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dimensionality-Reduction"><span class="nav-text">Dimensionality Reduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Principle-Component-Analysis"><span class="nav-text">Principle Component Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Locally-Linear-Embedding"><span class="nav-text">Locally Linear Embedding</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-8-Recommender-System"><span class="nav-text">Lecture 8. Recommender System</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Outline-7"><span class="nav-text">Outline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Overview-of-recommender-system-RS"><span class="nav-text">Overview of recommender system (RS)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-does-RS-do-recommendation"><span class="nav-text">How does RS do recommendation?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-to-build-a-RS"><span class="nav-text">How to build a RS?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Problem-Formulation"><span class="nav-text">Problem Formulation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Typical-methods"><span class="nav-text">Typical methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CF-based-method"><span class="nav-text">CF-based method</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Represented-by-correlation"><span class="nav-text">Represented by correlation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Represent-by-matrix-factorization"><span class="nav-text">Represent by matrix factorization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-9-Automated-Machine-Learning"><span class="nav-text">Lecture 9. Automated Machine Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Tuning-Hyper-parameters"><span class="nav-text">Tuning Hyper-parameters</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-10-Logical-Agents"><span class="nav-text">Lecture 10. Logical Agents</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Outline-8"><span class="nav-text">Outline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Knowledge-based-Agents"><span class="nav-text">Knowledge-based Agents</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Represent-Knowledge-with-Logic"><span class="nav-text">Represent Knowledge with Logic</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Inference-1"><span class="nav-text">Inference</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Propositional-Logic"><span class="nav-text">Propositional Logic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inference-with-Propositional-Logic"><span class="nav-text">Inference with Propositional Logic</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Inference-as-a-search-problem"><span class="nav-text">Inference as a search problem</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Proof-by-resolution"><span class="nav-text">Proof by resolution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Forward-chaining"><span class="nav-text">Forward chaining</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Backward-chaining"><span class="nav-text">Backward chaining</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DPLL"><span class="nav-text">DPLL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary-1"><span class="nav-text">Summary</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-11-First-Order-Logic"><span class="nav-text">Lecture 11. First Order Logic</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Inference-with-FOL"><span class="nav-text">Inference with FOL</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-12-Representing-and-Inference-with-Uncertainty"><span class="nav-text">Lecture 12. Representing and Inference with Uncertainty</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Outline-9"><span class="nav-text">Outline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Uncertainty-and-Rational-Decisions"><span class="nav-text">Uncertainty and Rational Decisions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Basic-Probability-Theory-and-Usage"><span class="nav-text">Basic Probability Theory and Usage</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bayesian-Networks"><span class="nav-text">Bayesian Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Inference-with-BN"><span class="nav-text">Inference with BN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Approximate-Inference-with-BN"><span class="nav-text">Approximate Inference with BN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Sampling-from-an-empty-network"><span class="nav-text">Sampling from an empty network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Rejection-Sampling"><span class="nav-text">Rejection Sampling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Likelihood-Weighting"><span class="nav-text">Likelihood Weighting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MCMC"><span class="nav-text">MCMC</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#How-to-construct-a-BN-or-KB-in-general"><span class="nav-text">How to construct a BN (or KB in general) ?</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-13-Knowledge-Graph"><span class="nav-text">Lecture 13. Knowledge Graph</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Overview-of-Knowledge-Graph-KG"><span class="nav-text">Overview of Knowledge Graph (KG)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-to-construct-Knowledge-Graph%EF%BC%9F"><span class="nav-text">How to construct Knowledge Graph？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Automatic-Entity-Recognition"><span class="nav-text">Automatic Entity Recognition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Automatic-Relation-Extraction"><span class="nav-text">Automatic Relation Extraction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Knowledge-Graph-Completion"><span class="nav-text">Knowledge Graph Completion</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KG-Based-Recommender-System"><span class="nav-text">KG-Based Recommender System</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Review-and-Semester-Summary"><span class="nav-text">Review and Semester Summary</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Connection-with-Previous-Courses"><span class="nav-text">Connection with Previous Courses</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Bionic l'Hôpital"
      src="/images/me.png">
  <p class="site-author-name" itemprop="name">Bionic l'Hôpital</p>
  <div class="site-description" itemprop="description">Ho Kai Kwan's Personal Pages</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/HQJ2221" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HQJ2221" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/about/" title="About-Me → &#x2F;about&#x2F;" rel="noopener me"><i class="fa fa-user-circle fa-fw"></i>About-Me</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="Back to top">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://hqj2221.github.io/2025/01/01/Artificial-Intelligence/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/me.png">
      <meta itemprop="name" content="Bionic l'Hôpital">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ho Kai Kwan's Personal Pages">
      <meta itemprop="description" content="Ho Kai Kwan's Personal Pages">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="CS303 人工智能 | Ho Kai Kwan's Personal Pages">
      <meta itemprop="description" content="The most basic AI knowledge, including some ML algorithm and concept of Agents.">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CS303 人工智能
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-01-01 22:05:22" itemprop="dateCreated datePublished" datetime="2025-01-01T22:05:22+08:00">2025-01-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-01-07 14:51:02" itemprop="dateModified" datetime="2025-01-07T14:51:02+08:00">2025-01-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/2024-Fall/" itemprop="url" rel="index"><span itemprop="name">2024 Fall</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">The most basic AI knowledge, including some ML algorithm and concept of Agents.</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><p><strong>AI Search</strong></p>
<ul>
<li>Lecture 1. AI as Search</li>
<li>Lecture 2. Beyond Classical Search</li>
<li>Lecture 3. Problem-Specific Search</li>
</ul>
<p><strong>Machine Learning</strong></p>
<ul>
<li>Lecture 4. Principles of Machine Learning</li>
<li>Lecture 5. Supervised Learning</li>
<li>Lecture 6. Performance Evaluation for Machine Learning</li>
<li>Lecture 7. Unsupervised Learning</li>
<li>Lecture 8. Recommender System</li>
<li>Lecture 9. Automated Machine Learning</li>
</ul>
<p><strong>Knowledge and Reasoning</strong></p>
<ul>
<li>Lecture 10. Logical Agents</li>
<li>Lecture 11. First Order Logic</li>
<li>Lecture 12. Representing and Inference with Uncertainty</li>
<li>Lecture 13. Knowledge Graph</li>
</ul>
<h1 id="Lecture-1-AI-as-Search"><a href="#Lecture-1-AI-as-Search" class="headerlink" title="Lecture 1. AI as Search"></a>Lecture 1. AI as Search</h1><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li>From searching to search tree</li>
<li>Uninformed Search Methods </li>
<li><font color="red">Heuristic (informed) Search</font> 



</li>
</ul>
<h2 id="Search-in-a-Tree"><a href="#Search-in-a-Tree" class="headerlink" title="Search in a Tree ?"></a>Search in a Tree ?</h2><blockquote>
<p><strong>概念：最短路径选择，搜索树</strong></p>
<p>通过将所有“图”中的路径展开，可以得到一个搜索树。对树进行全局搜索必然可以得到最短路径。</p>
<p>但是，当图变得复杂时（如一个省的地图，or a state），搜索量极大，全局搜索及其低能。</p>
</blockquote>
<ul>
<li><font color="red">Q:</font> What is A Good Search Method?<ul>
<li><font color="green">Completeness</font>: Does it always find a solution if it exists?</li>
<li><font color="green">Optimality</font>: Does it always find the least-cost solution?</li>
<li><font color="green">Time complexity</font>: # nodes generated/expanded.</li>
<li><font color="green">Space complexity</font>: maximum # nodes in memory.</li>
</ul>
</li>
<li>In general, time and space complexity depend on:<ul>
<li>$b$ 👉 maximum # successors of any node in search tree. [枝]</li>
<li>$d$ 👉 depth of the least-cost solution. </li>
<li>$m$ 👉 maximum length of any path in the state space.</li>
</ul>
</li>
</ul>
<h2 id="Un-informed-Search-Methods"><a href="#Un-informed-Search-Methods" class="headerlink" title="Un-informed Search Methods"></a>Un-informed Search Methods</h2><blockquote>
<p>How to define “un-informed” ?</p>
</blockquote>
<ul>
<li>Use only the information available in the problem definition.</li>
<li>Use <strong>NO</strong> problem-specific knowledge.</li>
</ul>
<blockquote>
<p>As some algorithms have been learnt in course Algorithm Design and Analysis, we skip them.</p>
</blockquote>
<h3 id="BFS"><a href="#BFS" class="headerlink" title="BFS"></a>BFS</h3><ul>
<li><font color="DodgerBlue">Completeness?</font> Yes (Suppose $b$ is finite)</li>
<li><font color="DodgerBlue">Optimality?</font> Yes (Suppose all edge values are <strong>non-negative</strong>)</li>
<li><font color="DodgerBlue">Time &amp; Space?</font> Both $O(b^{d+1})$</li>
</ul>
<h3 id="UCS-Uniform-Cost-Search"><a href="#UCS-Uniform-Cost-Search" class="headerlink" title="UCS (Uniform-Cost Search)"></a>UCS (Uniform-Cost Search)</h3><ul>
<li>Idea<ul>
<li>Expand the cheapest unexpanded node.</li>
<li>Implementation: a queue ordered by path cost, lowest first.</li>
</ul>
</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai101.png" style="zoom:60%"></p>
<ul>
<li><font color="DodgerBlue">Completeness?</font> Yes (Suppose every step costs $\ge \epsilon$)</li>
<li><font color="DodgerBlue">Optimality?</font> Yes (Suppose all edge values are <strong>non-negative</strong>)</li>
<li><font color="DodgerBlue">Time &amp; Space?</font>  $O(b^{1+\lfloor C^{*}/\epsilon\rfloor})$<ul>
<li>$C^{*}$ : the cost of the optimal solution.</li>
<li>every action costs at least $\epsilon$.</li>
<li>Only if all step costs are equal, time/space $=O(b^{d+1})$</li>
</ul>
</li>
</ul>
<h3 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a>DFS</h3><ul>
<li><font color="DodgerBlue">Completeness?</font> No (fail in infinite-depth space and space with loops.)</li>
<li><font color="DodgerBlue">Optimality?</font> No</li>
<li><font color="DodgerBlue">Time?</font> $O(b^m)$<ul>
<li>Terrible if $m$ is much larger than $d$.</li>
</ul>
</li>
<li><font color="DodgerBlue">Space?</font> $O(bm)$ - linear!</li>
</ul>
<h3 id="DLS-Depth-Limit-Search"><a href="#DLS-Depth-Limit-Search" class="headerlink" title="DLS (Depth-Limit Search)"></a>DLS (Depth-Limit Search)</h3><ul>
<li>A variant of DFS: node at depth $l$ has no sucessors.</li>
<li><font color="DodgerBlue">Completeness?</font> No</li>
<li><font color="DodgerBlue">Optimality?</font> No</li>
<li><font color="DodgerBlue">Time?</font> $O(b^l)$</li>
<li><font color="DodgerBlue">Space?</font> $O(bl)$</li>
</ul>
<h3 id="IDS-Iterative-Deepening-Search"><a href="#IDS-Iterative-Deepening-Search" class="headerlink" title="IDS (Iterative Deepening Search)"></a>IDS (Iterative Deepening Search)</h3><ul>
<li>Idea<ul>
<li>Apply <strong>DLS</strong> with increasing limits</li>
<li>Combine benefit of <strong>BFS</strong> and <strong>DFS</strong></li>
</ul>
</li>
<li><font color="DodgerBlue">Completeness?</font> Yes</li>
<li><font color="DodgerBlue">Optimality?</font> Yes (Suppose costs of edges are non-negative)</li>
<li><font color="DodgerBlue">Time?</font> $O(b^d)$<ul>
<li>$(d+1)b^{0}+ db^{1}+(d-1)b^{2}+\cdots +b^d=O(b^{d})$</li>
</ul>
</li>
<li><font color="DodgerBlue">Space?</font> $O(bd)$</li>
</ul>
<blockquote>
<p>Preference when <strong>search space</strong> is large and <strong>depth</strong> of solution is unknown.</p>
</blockquote>
<h3 id="Bi-directional-Search"><a href="#Bi-directional-Search" class="headerlink" title="Bi-directional Search"></a>Bi-directional Search</h3><ul>
<li>Idea: simultaneous<ul>
<li>Replace single search tree with two smaller sub trees.</li>
<li>Forward tree: forward search from source to goal.</li>
<li>Backward tree: backward search from goal to source.</li>
</ul>
</li>
<li><font color="DodgerBlue">Completeness &amp; Optimality?</font> Like BFS (if BFS used in both trees)</li>
<li><font color="DodgerBlue">Time &amp; Space?</font> $O(b^{d/2})$</li>
<li>Cons: not always applicable<ul>
<li>Reversible actions? [是否可以“由果溯因”？]</li>
<li>Explicitly stated goal state? [叶子节点代表“结局”，所有结局是否已知？]</li>
</ul>
</li>
</ul>
<h2 id="Heuristic-informed-Search"><a href="#Heuristic-informed-Search" class="headerlink" title="Heuristic (informed) Search"></a><font color="red">Heuristic (informed) Search</font></h2><blockquote>
<p><font color="red">Q:</font> What is “Heuristic” ?</p>
<p><font color="green">A:</font> Based on algorithm design, it searches the trees with intelligence, making use of <strong>domain knowledge</strong>.</p>
<p><font color="red">Q:</font> How to Design the Evaluation Function?</p>
<p><font color="green">A:</font> It depends (but some advice). Use heuristic function $f(x)$ to estimates the cheapest cost from $x$ to the goal state.</p>
<ol>
<li>$h(x)=0$ if $x$ is the goal state</li>
<li>non-negative</li>
<li><strong>problem-specific</strong></li>
</ol>
</blockquote>
<h3 id="Greedy-Best-first-Search"><a href="#Greedy-Best-first-Search" class="headerlink" title="Greedy Best-first Search"></a>Greedy Best-first Search</h3><ul>
<li>A simple example to describe heuristic function: let’s say $f(x)=h_{SLD}(x)$ , where $h_{SLD}(x)$ is physical distance between two nodes (cities)</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai102.png" style="zoom:60%"></p>
<ul>
<li><font color="DodgerBlue">Completeness?</font> Yes (finite space + repeated-state checking)</li>
<li><font color="DodgerBlue">Optimality?</font> No</li>
<li><font color="DodgerBlue">Time?</font> $O(b^m)$ (In practice, good heuristic gives drastic improvement)</li>
<li><font color="DodgerBlue">Space?</font> $O(b^m)$</li>
</ul>
<h3 id="A-Search"><a href="#A-Search" class="headerlink" title="A* Search"></a>A* Search</h3><ul>
<li>Idea: avoid expanding paths that are already expensive.<ul>
<li>Expand the node $x$ that has minimal $f(x)=h(x)+g(x)$<ul>
<li>$g(x)$ : cost so far to reach $x$</li>
<li>$h(x)$ : estimated cost from $x$ to goal</li>
<li>$f(x)$ : total cost</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>PF(performance) Metrics</strong></p>
<ul>
<li><font color="DodgerBlue">Completeness?</font> Yes</li>
<li><font color="DodgerBlue">Optimality?</font> Yes, if $h$ is <strong>admissible</strong> [取决于启发式算法]</li>
<li><font color="DodgerBlue">Time?</font> $O(b^d)$</li>
<li><font color="DodgerBlue">Space?</font> $O(b^d)$</li>
</ul>
<p><strong>Admissible heuristic</strong></p>
<ul>
<li><strong>Def.</strong> Heuristic function $h$ is admissible if $\forall x \to h(x)\ge h’(x)$, where $h’(x)$ is the <strong>true</strong> cost from $x$ to goal.</li>
<li>Search Efficiency of Admissible Heuristic<ul>
<li>For admissible $h_1$ and $h_2$, if $h_2(x)\ge h_1(x)$ for all $n$, then $h_2$ <strong>deminates</strong> $h_1$ and is more efficient for search.</li>
</ul>
</li>
</ul>
<blockquote>
<p>A* 算法可以认为是学习 AI 的第一基础算法。它明确了 AI 需要具有的首要特性：学习。A* 算法是树/图搜索算法中第一个可以利用“知识”进行决策的算法，不再是像遍历这样“机械、随机”的算法。</p>
</blockquote>
<h1 id="Lecture-2-Beyond-Classical-Search"><a href="#Lecture-2-Beyond-Classical-Search" class="headerlink" title="Lecture 2. Beyond Classical Search"></a>Lecture 2. Beyond Classical Search</h1><h2 id="Outline-1"><a href="#Outline-1" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li>More representations</li>
<li>General Search Frameworks</li>
<li>Summary</li>
</ul>
<h2 id="More-representations"><a href="#More-representations" class="headerlink" title="More representations"></a>More representations</h2><blockquote>
<p>Last lecture, we solve the search problem by Searching Tree.</p>
<p>But is there any more efficient data structure in some specific tasks ?</p>
</blockquote>
<h3 id="Direct-Search-in-the-Solution-Space"><a href="#Direct-Search-in-the-Solution-Space" class="headerlink" title="Direct Search in the Solution Space"></a>Direct Search in the Solution Space</h3><ul>
<li>Consider that the solution space is continuous, like $\mathbb{R}^{2}$.</li>
<li>Then if we generate a tree to describe each step, that’s silly.</li>
</ul>
<hr>
<ul>
<li>Representations of a solution space can be roughly categorized as:<ul>
<li>Continuous</li>
<li>Discrete: Binary, Integer, Permutation, etc.</li>
</ul>
</li>
<li>Different representations may favor different search methods, but most of them share a <strong>common framework</strong>.</li>
</ul>
<h2 id="General-Search-Frameworks"><a href="#General-Search-Frameworks" class="headerlink" title="General Search Frameworks"></a>General Search Frameworks</h2><p><img src="/2025/01/01/Artificial-Intelligence/ai201.png"></p>
<ul>
<li>Typical Frameworks:<ul>
<li>Local Search</li>
<li>Simulated Annealing</li>
<li>Tabu Search</li>
<li>Population-based search</li>
</ul>
</li>
<li>Two basic issues (differs over concrete search methods): <ul>
<li>search operator (how to generate a new candidate solution)</li>
<li>evaluation criterion (or replacement strategy)</li>
</ul>
</li>
</ul>
<h3 id="Typical-Search-Operators"><a href="#Typical-Search-Operators" class="headerlink" title="Typical Search Operators"></a>Typical Search Operators</h3><ul>
<li>A Search Operator generate a new solution based on previous ones.</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>\phi : x\to x’,\forall x,x’ \in \mathcal{X}<br>$</p>

</blockquote>
<blockquote>
<p>Now we use continuous case as an example.</p>
</blockquote>
<h3 id="Greedy-Local-Search-Framework"><a href="#Greedy-Local-Search-Framework" class="headerlink" title="Greedy Local Search Framework"></a>Greedy Local Search Framework</h3><ul>
<li>Given a predefined Local Search Operator</li>
<li>Iteratively generate new solutions</li>
<li>Always pick the best solution so far, sometimes also known as <font color="DodgerBlue">Hill Climbing</font>.</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai202.png"></p>
<blockquote>
<p>but sometimes trapped in local optimum</p>
</blockquote>
<h3 id="SA-Tabu-and-Bayesian-Optimizations"><a href="#SA-Tabu-and-Bayesian-Optimizations" class="headerlink" title="SA, Tabu and Bayesian Optimizations"></a>SA, Tabu and Bayesian Optimizations</h3><h4 id="Simulated-Annealing"><a href="#Simulated-Annealing" class="headerlink" title="Simulated Annealing"></a>Simulated Annealing</h4><blockquote>
<p>概念：温度，下降率</p>
<p>模拟退火是基于物理退火现象的一种对于贪心策略的优化方法。目的是在一定概率上帮助跳出局部最优的局面。（随机）</p>
<p>该优化方法需要根据实际情况调整超参数：初始温度 $T_0$，结束温度 $T_t$，和温度下降率 $\alpha$。</p>
</blockquote>
<blockquote class="blockquote-center">
<p>$<br>p=\left\{<br>\begin{array}{cl}<br>&amp;1 &amp;\text{if } f(x_i) \lt f(x_i’) \\<br>&amp;\exp{-\frac{f(x_i)-f(x_i’)}{T}} &amp;\text{if } f(x_i) \lt f(x_i’)<br>\end{array}\right.<br>$</p>

</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Set</span> T(<span class="number">0</span>), T(t), alpha</span><br><span class="line">init x(<span class="number">0</span>), T(i) = T(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">while</span> T(i) &lt; T(t):</span><br><span class="line">  generate x‘ based on x(i)</span><br><span class="line">  calc f(x‘)</span><br><span class="line">  calc p</span><br><span class="line">  <span class="keyword">if</span> c = random[<span class="number">0</span>,<span class="number">1</span>] &lt; p: x(i+<span class="number">1</span>) = x‘</span><br><span class="line">  <span class="keyword">else</span>: x(i+<span class="number">1</span>) = x(i)</span><br><span class="line">  i += <span class="number">1</span></span><br><span class="line">  Update T(i) = T(i) * alpha</span><br><span class="line">end</span><br><span class="line"><span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="Tabu"><a href="#Tabu" class="headerlink" title="Tabu"></a>Tabu</h4><p><strong>Hill climbing → Tabu Search</strong></p>
<ul>
<li>Key idea: Don’t visit the sample candidate solution twice.</li>
<li>Challenge: How to define the Tabu list?</li>
<li>Concept:<ul>
<li>Tabu List [禁忌表]</li>
<li>Tabu Object: items in TL, e.g., in Traveling Salesman Problem (TSP), we can set cities as TO (can be some attributes involved to $f(x)$)</li>
<li>Tabu Tenure: “Time” that TO stay in TL. 👉 to avoid short loop(TT↓) or low PF(TT↑)</li>
<li>Aspiration Criteria: to choose TO with best PF, and pop it out from TL.</li>
</ul>
</li>
</ul>
<blockquote>
<p>通过维持一个禁忌表的方式，在一定程度上接收比当前最优解要差的结果，从而跳出局部最优</p>
</blockquote>
<h4 id="Bayesian-Optimizations"><a href="#Bayesian-Optimizations" class="headerlink" title="Bayesian Optimizations"></a>Bayesian Optimizations</h4><p><strong>Hill climbing → Bayesian Optimizations</strong></p>
<ul>
<li>Key idea: Build a model to ”guess” which solution is good.</li>
<li>Challenge: <ul>
<li>model building is non-trivial</li>
<li>may need lots of data to build the model, limited to low-dimensional problem</li>
</ul>
</li>
</ul>
<blockquote>
<p>涉及机器学习内容</p>
</blockquote>
<h3 id="Population-based-Search"><a href="#Population-based-Search" class="headerlink" title="Population-based Search"></a>Population-based Search</h3><ul>
<li>Idea: Since we sample from a probability distribution, why 1 at a time?</li>
<li><font color="red">Evolutionary Algorithm</font>:</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai203.png"></p>
<ul>
<li>Seeking a good distribution: maximize the following “objective function”:</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>\mathcal{J}=\int f(x)p(x|\theta_{1}) dx<br>$</p>

</blockquote>
<ul>
<li>where $p(x|\theta_{1})$ is prob density function parameterized by $\theta_{1}$</li>
<li>Using “Population” help making objective function “smooth”</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai204.png"></p>
<ul>
<li>Suppose we now converge to some (global or local) optimum</li>
<li>If run the algorithm again, we hope the algorithm (i.e., the distribution corresponding to the final population) <font color="red">converge to a different optimum</font> (and thus a different PDF).</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>\mathcal{J}= \sum_{i=1}^{\lambda} \int f(x)p(x|\theta_{i}) dx - \sum_{i=1}^{\lambda} \sum_{j=1}^{\lambda} C(\theta_{i}, \theta_{j})<br>$</p>

</blockquote>
<ul>
<li>where $C(\theta_{i},\theta_{j})$ is similarity of the two PDFs [概率密度分布]</li>
</ul>
<p><strong>EA Applications</strong></p>
<ul>
<li>N-Queen Problems [通过“杂交”操作生成新组合，每次进行多个组合计算]</li>
<li>鸟巢设计，动车车头设计等</li>
</ul>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>This lecture is talking about <font color="red">general-purpose search frameworks</font>, rather than search algorithm.</li>
<li>When addressing a specific problem, heuristics derived from domain knowledge needs to be incorporated in forms of search operators to obtain the best performance.</li>
<li>For some problem of great importance, mature <font color="red">application-specific optimization approaches</font> have been developed such that algorithm design from scratch is not needed.</li>
</ul>
<blockquote>
<p>Therefore, next lecture will talk about some problem-specific search algorithms.</p>
</blockquote>
<h1 id="Lecture-3-Problem-Specific-Search"><a href="#Lecture-3-Problem-Specific-Search" class="headerlink" title="Lecture 3. Problem-Specific Search"></a>Lecture 3. Problem-Specific Search</h1><h2 id="Outline-2"><a href="#Outline-2" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li>Make Search Algorithms Less General</li>
<li>Gradient-based Methods for Numerical Optimization</li>
<li>Quadratic Programming Problems</li>
<li>Constraint Satisfaction Problems</li>
<li>Adversarial Search</li>
</ul>
<h2 id="Make-Search-Algorithms-Less-General"><a href="#Make-Search-Algorithms-Less-General" class="headerlink" title="Make Search Algorithms Less General"></a>Make Search Algorithms Less General</h2><blockquote>
<p>Why we need problem-specific search ?</p>
</blockquote>
<ul>
<li>When designing an algorithm for a problem (class), taking the problem characteristics into account usually helps us get the desired solution by <font color="red">searching only a part of the search/state space</font>, making the search more efficient.</li>
</ul>
<font color="green"><b>Recall</b></font>

<ul>
<li>consider the ubiquitous (普遍的) optimization problems:</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>\begin{array}{ll}<br>&amp;\text{maximize} &amp;f(x) \\<br>&amp;\text{subject to:} &amp;g_i(x)\le 0,\ i=1\cdots m \\<br>&amp; &amp;h_j(x)=0,\ j=1\cdots p<br>\end{array}<br>$</p>

</blockquote>
<ul>
<li>What is “problem characteristic”? Most basically:<ul>
<li>What is $x$ ?</li>
<li>What is $f$ ?</li>
<li>Does $f$ fulfill some properties that would lead to a more efficient search?</li>
</ul>
</li>
</ul>
<h2 id="Gradient-based-Methods-for-Numerical-Optimization"><a href="#Gradient-based-Methods-for-Numerical-Optimization" class="headerlink" title="Gradient-based Methods for Numerical Optimization"></a>Gradient-based Methods for Numerical Optimization</h2><ul>
<li>Suppose the objective function $f(x_1 , y_1, x_2, y_2, x_3, y_3)$ is continuous and differentiable (thus the gradient could be calculated)</li>
<li>Compute:</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>\nabla f=\left( \large{\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial y_1}, \frac{\partial f}{\partial x_2}, \frac{\partial f}{\partial y_2}, \frac{\partial f}{\partial x_3}, \frac{\partial f}{\partial y_3}} \right)<br>$</p>

</blockquote>
<ul>
<li>to increase/reduce $f$, e.g., by $x \leftarrow x + \alpha \nabla f(x)$ [梯度下降]</li>
</ul>
<h2 id="Quadratic-Programming-Problems"><a href="#Quadratic-Programming-Problems" class="headerlink" title="Quadratic Programming Problems"></a>Quadratic Programming Problems</h2><ul>
<li>The objective function is a <font color="red">quadratic(二次) function</font> of $x$</li>
<li>The constraints are linear functions of $x$</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>\begin{array}{ll}<br>&amp; &amp;\min f(x)=q^{T}x + \frac{1}{2}x^{T}Qx \\<br>&amp;\text{s.t.} &amp; Ax = a,\ Bx \le b,\ x\ge 0 \\<br>\end{array}<br>$</p>

</blockquote>
<ul>
<li>When there is <strong>no constraint</strong>, we can solve this problem by differenctiation. ($f’(x)=0$)</li>
<li>But when there are constraints, search is still needed (<font color="green">Recall:</font> Lagrange multiplier)</li>
</ul>
<blockquote>
<p>引出接下来的问题：CSP</p>
</blockquote>
<h2 id="Constraint-Satisfaction-Problems-CSP"><a href="#Constraint-Satisfaction-Problems-CSP" class="headerlink" title="Constraint Satisfaction Problems (CSP)"></a><font color="red">Constraint Satisfaction Problems (CSP)</font></h2><ul>
<li>Standard Search Problem<ul>
<li><strong>state</strong> is a “black box” 👉 any old data structure that supports goal test, eval, successors</li>
</ul>
</li>
<li>CSP<ul>
<li><strong>state</strong> is defined by variable $X_i$ with values from domain $D_i$</li>
<li><strong>goal test</strong> is a set of constraints specifying allowable combinations of values for subsets of variables</li>
</ul>
</li>
</ul>
<blockquote>
<p>例：地图上色问题。限制：相邻区块不能上同一颜色。</p>
<p>假设两个区块 $X=\{A,B\}$，四种颜色 $D=\{R, G, B, Y\}$，原本可以有 16 种组合，但是由于限制条件只有 12 种。</p>
</blockquote>
<h3 id="Characteristics-of-CSPs"><a href="#Characteristics-of-CSPs" class="headerlink" title="Characteristics of CSPs"></a>Characteristics of CSPs</h3><h4 id="Real-world-CSPs"><a href="#Real-world-CSPs" class="headerlink" title="Real-world CSPs"></a>Real-world CSPs</h4><ul>
<li>Assignment problems</li>
<li>Timetabling problems</li>
<li>Hardware configuration</li>
<li>Floorplanning</li>
<li>Factory scheduling</li>
<li>……</li>
</ul>
<h4 id="Commutativity"><a href="#Commutativity" class="headerlink" title="Commutativity"></a>Commutativity</h4><blockquote>
<p>First character</p>
</blockquote>
<ul>
<li>Commutativity help us formulate the search tree (only 1 variable needs to be considered at each node in the search tree).</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai301.png"></p>
<h4 id="Constraint-Graph"><a href="#Constraint-Graph" class="headerlink" title="Constraint Graph"></a>Constraint Graph</h4><blockquote>
<p>Second</p>
</blockquote>
<p><img src="/2025/01/01/Artificial-Intelligence/ai302.png"></p>
<h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><ul>
<li>A constraint graph allows the agent to do <font color="red">inference</font> in addition to search. </li>
<li>Inference basically means <font color="red">checking local consistency</font> (or detecting inconsistency) <ul>
<li>Node consistency</li>
<li>Arc Consistency</li>
<li>Path Consistency</li>
<li>K-consistency</li>
<li>Global consistency</li>
</ul>
</li>
<li>Inference helps <font color="red">prune</font> the search tree, either before or during the search.</li>
</ul>
<h3 id="Backtracking-Search-for-CSP"><a href="#Backtracking-Search-for-CSP" class="headerlink" title="Backtracking Search for CSP"></a>Backtracking Search for CSP</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backtracking_search</span>(<span class="params">csp</span>) -&gt; (solution/failure):</span><br><span class="line">  <span class="keyword">return</span> recursive_backtracking(&#123;&#125;, csp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">recursive_backtracking</span>(<span class="params">&#123;&#125;, csp</span>) -&gt; (result/failure):</span><br><span class="line">  <span class="keyword">if</span> assignment <span class="keyword">is</span> complete then <span class="keyword">return</span> assignment</span><br><span class="line">  var &lt;- Select_Unassigned_Variable(Variable[csp], assignment, csp)</span><br><span class="line">  <span class="keyword">for</span> value <span class="keyword">in</span> Order_Domain_Value(var, assignment, csp) do</span><br><span class="line">    <span class="keyword">if</span> value <span class="keyword">is</span> consistent <span class="keyword">with</span> assignment given constraint[csp] then</span><br><span class="line">      add &#123;var = value&#125; to assignment</span><br><span class="line">      result &lt;- recursive_backtracking(assignment, csp)</span><br><span class="line">      <span class="keyword">if</span> result != failure then <span class="keyword">return</span> result</span><br><span class="line">      remove &#123;var = value&#125; <span class="keyword">from</span> assignment</span><br><span class="line">  <span class="keyword">return</span> failure</span><br></pre></td></tr></table></figure>
<ul>
<li>More improvement can be done:<ul>
<li>E.g. in <code>Select_Unassigned_Variable</code>, design some strategies to choose the optimal variable</li>
<li>E.g. maintain a conflict set, etc.</li>
</ul>
</li>
</ul>
<h2 id="Adversarial-对抗性-Search"><a href="#Adversarial-对抗性-Search" class="headerlink" title="Adversarial(对抗性) Search"></a>Adversarial(对抗性) Search</h2><blockquote>
<p>以一个游戏为例：</p>
</blockquote>
<p><img src="/2025/01/01/Artificial-Intelligence/ai303.png"></p>
<p><strong>Algorithm.</strong> Minimax Algorithm</p>
<ul>
<li>Idea<ul>
<li>Assume the game is deterministic and perfect information is available</li>
<li>For player “MAX”, choose the move to position the <font color="dodgerblue">highest minimax value</font></li>
</ul>
</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai304.png"></p>
<ul>
<li>Perform a <strong>complete</strong>(later we will optimize this) depth-first search of the game tree.</li>
<li><strong>Recursively</strong> compute the minimax values of each successor state.</li>
<li>Maximize the worst-case outcome for MAX.</li>
</ul>
<h3 id="Alpha-Beta-Pruning"><a href="#Alpha-Beta-Pruning" class="headerlink" title="Alpha-Beta Pruning"></a>Alpha-Beta Pruning</h3><ul>
<li>Idea: Remove (unneeded) part of the minimax tree from consideration.</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai305.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># AB Search</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">alpha_beta_search</span>(<span class="params">state</span>):</span><br><span class="line">  <span class="keyword">return</span> max_value(state, -INF, INF)</span><br><span class="line"></span><br><span class="line"><span class="comment"># MAX search</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">max_value</span>(<span class="params">state, alpha, beta</span>):</span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">is</span> terminal state:</span><br><span class="line">    <span class="keyword">return</span> util(state)</span><br><span class="line">  v &lt;- (-INF)</span><br><span class="line">  <span class="keyword">for</span> a <span class="keyword">in</span> action(state):</span><br><span class="line">    v &lt;- <span class="built_in">max</span>(v, min_value(result(s, a), alpha, beta))</span><br><span class="line">    <span class="keyword">if</span> v &gt;= beta:  <span class="comment"># pruning</span></span><br><span class="line">      <span class="keyword">return</span> v</span><br><span class="line">    alpha &lt;- <span class="built_in">max</span>(alpha, v)</span><br><span class="line">  <span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line"><span class="comment"># MIN search</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">min_value</span>(<span class="params">state, alpha, beta</span>):</span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">is</span> terminal state:</span><br><span class="line">    <span class="keyword">return</span> util(state)</span><br><span class="line">  v &lt;- INF</span><br><span class="line">  <span class="keyword">for</span> a <span class="keyword">in</span> action(state):</span><br><span class="line">    v &lt;- <span class="built_in">min</span>(v, max_value(result(s, a), alpha, beta))</span><br><span class="line">    <span class="keyword">if</span> v &lt;= alpha:  <span class="comment"># pruning</span></span><br><span class="line">      <span class="keyword">return</span> v</span><br><span class="line">    beta &lt;- <span class="built_in">min</span>(beta, v)</span><br><span class="line">  <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure>
<ul>
<li>Explanation:<ul>
<li>when we search maximum for “MAX”, we first suppose “MAX” choose action a, and “MIN” make perfect action (minimum in “MAX” view), assuming that value is 3.</li>
<li>Then “MAX” will focus on range $[3, \infty]$.</li>
<li>We then suppose “MAX” choose action b, and “MIN” make a choice for an action value 2.</li>
<li>We’re sure that “MIN” ultimately will make a choice having value $\le 2$ (optimal for “MIN”). However, “MAX” only accept actions value $\ge 3$, so action b won’t be accepted.</li>
<li>Therefore, we can stop “MIN” from searching the other actions.</li>
</ul>
</li>
<li>So we found that Alpha-Beta Pruning have limitation:<ul>
<li>games with more than 2 players ?</li>
<li>2-players game that is not zero-sum ? [非敌对？]</li>
<li>Minimax or Alpha-Beta Pruning don’t apply ?</li>
</ul>
</li>
<li>And sometimes the <strong>order</strong> of “actions” affects the PF.</li>
</ul>
<h2 id="Summary-on-Search"><a href="#Summary-on-Search" class="headerlink" title="Summary on Search"></a>Summary on Search</h2><ul>
<li>How to <font color="red">represent</font> the search space?<ul>
<li>Search Tree (state space)</li>
<li>Solution space</li>
</ul>
</li>
<li>What is the <font color="red">objective function and constraint</font>, and algorithm in textbook already good enough? </li>
<li>Which <font color="red">algorithmic framework</font> to choose?<ul>
<li>Tree search, e.g., Un-informed Search, Heuristic Search (A*…)</li>
<li>Direct search in the solution space, e.g., Hill Climbing, Simulated Annealing, Genetic Algorithm… </li>
</ul>
</li>
<li>How to define <font color="red">concrete components</font> of the algorithm framework? <ul>
<li>General-purpose operators in literature</li>
<li>Problem-specific operators, designed based on domain knowledge</li>
</ul>
</li>
</ul>
<blockquote>
<p>Always <strong>trade-off</strong> among solution quality, efficiency, and your domain knowledge</p>
</blockquote>
<h1 id="Lecture-4-Principles-of-Machine-Learning"><a href="#Lecture-4-Principles-of-Machine-Learning" class="headerlink" title="Lecture 4. Principles of Machine Learning"></a>Lecture 4. Principles of Machine Learning</h1><h2 id="Outline-3"><a href="#Outline-3" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li>What is Learning</li>
<li>Key Questions for Learning</li>
<li>Learning Paradigms and Principles</li>
</ul>
<h2 id="What-is-Learning"><a href="#What-is-Learning" class="headerlink" title="What is Learning?"></a>What is Learning?</h2><ul>
<li><strong>Machine Learning</strong>: Given some observations (data) from the environment, how could an agent improve its agent function?</li>
<li>Intuitive assumptions<ul>
<li>the data share something in common</li>
<li>“something” could be obtained by an algorithm/program</li>
</ul>
</li>
</ul>
<h3 id="Two-simple-methods"><a href="#Two-simple-methods" class="headerlink" title="Two simple methods"></a>Two simple methods</h3><p><strong>A Naive Parametric Method —— Bayesian Formula</strong></p>
<ul>
<li>Classify a data to the class with the highest posterior probability</li>
<li><font color="dodgerblue">Assumption:</font> data follows independent identically distribution</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>\begin{array}{c}<br>&amp;P(w_j|x)=\frac{p(x|w_j)p(w_j)}{p(x)} \\<br>&amp;P(w_2|x)\gt P(w_1|x) \Leftrightarrow \ln{p(x|w_2)}+\ln{p(w_2)} \gt \ln{p(x|w_1)}+\ln{p(w_1)} \\<br>\end{array}<br>$</p>

</blockquote>
<ul>
<li><strong>“Parametric”</strong>: the assumption on the probability density function (PDF).</li>
<li>Parametric methods usually do not involve parameters to fine-tune, while Nonparametric methods usually do.</li>
</ul>
<p><strong>A Linear Function</strong></p>
<ul>
<li>Find a straight line/hyper-plane to separate data from different classes.</li>
</ul>
<p><a name="ai401"><img src="/2025/01/01/Artificial-Intelligence/ai401.png"></a></p>
<h2 id="Key-Questions-for-Machine-Learning"><a href="#Key-Questions-for-Machine-Learning" class="headerlink" title="Key Questions for Machine Learning"></a>Key Questions for Machine Learning</h2><ul>
<li>What is the format of the data? (data representation)</li>
<li>What does the agent function look like? (model representation)</li>
<li>How to measure the “improvement”? (objective function)</li>
<li>What is the learning algorithm? (to get a good agent function)</li>
</ul>
<center><b>Representation + Algorithm + Evaluation = Agent function/Model</b></center>



<h2 id="Learning-Paradigms-and-Principles"><a href="#Learning-Paradigms-and-Principles" class="headerlink" title="Learning Paradigms and Principles"></a>Learning Paradigms and Principles</h2><ul>
<li>Learning Principles: <font color="magenta">Generalization!</font><ul>
<li>the learned agent function is expected to be able to handle previously unseen situations.</li>
</ul>
</li>
<li>Learning Paradigms (范式)<ul>
<li>A Machine Learning process typically involves two phases<ul>
<li>Training: build the agent function</li>
<li>Testing/Inference: test the agent function/deploy the agent function in real use.</li>
</ul>
</li>
<li>Different ML techniques may use different training/learning paradigms<ul>
<li><font color="red">Supervised Learning:</font> the correct answer is available to the learning algorithm.</li>
<li><font color="red">Reinforcement Learning:</font> the only feedback is the reward of an output, e.g., the output is correct or not (the correct answer is not given).</li>
<li><font color="red">Unsupervised Learning:</font> no correct answer is available</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="Lecture-5-Supervised-Learning"><a href="#Lecture-5-Supervised-Learning" class="headerlink" title="Lecture 5. Supervised Learning"></a>Lecture 5. Supervised Learning</h1><blockquote>
<p>注意：以下内容部分与课程 MA234 大数据导论与实践相重合！</p>
</blockquote>
<h2 id="Outline-4"><a href="#Outline-4" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li>LDA</li>
<li>SVM</li>
<li>ANN (NN)</li>
<li>DT</li>
</ul>
<h2 id="Linear-Discriminant-Analysis"><a href="#Linear-Discriminant-Analysis" class="headerlink" title="Linear Discriminant Analysis"></a>Linear Discriminant Analysis</h2><ul>
<li>Idea: Viewing each datum to lie in a Euclidean space, find a straight line (a linear function) in the space (recall the <a href="#ai401">example</a> used in the last lecture), where the data projection on this line/plane can be well separate according to their label.</li>
<li>Let’s learn some Maths:<ul>
<li>Given dataset $D=\{ (\mathbf{x}_i, y_i) \}^{m}_{i=1}$, $y_i \in \{0,1\}$</li>
<li>Suppose $X_i$ is the data subset with label $i\in \{0,1\}$, $\mathbf{\mu_i}$ is the mean vector, $\Sigma_i$ is the Covariance Matrix</li>
</ul>
</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai501.png" style="zoom:50%"></p>
<ul>
<li>Then the projection of samples in two classes are $w^T \mu_0$, $w^T \mu_1$</li>
<li>And the covariance within two classes are $w^T\Sigma_0 w$, $w^T\Sigma_1 w$</li>
<li>Trying to make projections of data in the same class closer, and in different classes farther, we describe the objective function $J$ in this way:</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>\begin{array}{rcl}<br>\text{define within-class scatter matrix:} &amp;\mathbf{S_w}&amp;=\mathbf{\Sigma_0}+\mathbf{\Sigma_1} \\<br>&amp;&amp;=\sum_{x\in X_0}(\mathbf{x}-\mathbf{\mu_0})^T+\sum_{x\in X_1}(\mathbf{x}-\mathbf{\mu_1})^T \\<br>\text{define between-class scatter matrix:}&amp;\mathbf{S_b}&amp;=(\mathbf{\mu_0}-\mathbf{\mu_1})(\mathbf{\mu_0}-\mathbf{\mu_1})^T \\<br>\text{Then we get objective function:}&amp;J&amp;=\large\frac{|| w^T\mathbf{\mu_0}-w^T\mathbf{\mu_1} ||^{2}_{2}}{w^T\Sigma_0 w+w^T\Sigma_1 w} \\<br>&amp;&amp;=\large\frac{w^T (\mathbf{\mu_0}-\mathbf{\mu_1})(\mathbf{\mu_0}-\mathbf{\mu_1})^T w}{w^T (\Sigma_0+\Sigma_1)w} \\<br>&amp;&amp;=\large\frac{w^T \mathbf{S_b} w}{w^T \mathbf{S_w} w}<br>\end{array}<br>$</p>

</blockquote>
<ul>
<li>So we get what LDA wants to maximize (also called <strong>generalized Rayleigh quotient</strong>)</li>
<li>then we’re going to optimize this function to obtain an easy form:</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>\begin{array}{rl}<br>&amp;\min_{\mathbf{w}} -\mathbf{w}^T\mathbf{S_b} \mathbf{w} \\<br>&amp;s.t.\ \mathbf{w}^T\mathbf{S_w} \mathbf{w}=1 \\<br>\text{use Langrange Multiplexer: }&amp;\mathbf{S_b}\mathbf{w}=\lambda\mathbf{S_w}\mathbf{w} \\<br>\text{Aware that } &amp;\mathbf{S_b}\mathbf{w} \text{ always has same direnction as } (\mathbf{\mu_0}-\mathbf{\mu_1}) \\<br>\text{Let }&amp;\mathbf{S_b}\mathbf{w} = \lambda (\mathbf{\mu_0}-\mathbf{\mu_1}) \\<br>\therefore&amp;\mathbf{w}=\mathbf{S_w}^{-1} (\mathbf{\mu_0}-\mathbf{\mu_1})<br>\end{array}<br>$</p>

</blockquote>
<blockquote>
<p>In practice, it’s more likely to represent $\mathbf{S_w}$ as form of Singularity Decomposition: $\mathbf{U}\mathbf{\Sigma}\mathbf{V}^T$.</p>
<p>So that we can get $\mathbf{S_w}^{-1}$ by computing $\mathbf{S_w}^{-1}= \mathbf{V}\mathbf{\Sigma}^{-1}\mathbf{U}^T$</p>
</blockquote>
<ul>
<li>This method is also practical in multi-classification task<ul>
<li>by computing $\mathbf{W}\in \mathbb{R}^{d\times (N-1)}$</li>
<li>Objective function: $\max_{W} \large\frac{tr(W^T S_b W)}{tr(W^T S_w W)}$</li>
</ul>
</li>
</ul>
<h2 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h2><ul>
<li>Basic idea: margin maximization<ul>
<li>the <font color="red">minimum</font> distance between a data point to the decision boundary is <font color="red">maximized</font>.</li>
<li>intuitively, the safest and most robust</li>
<li><font color="red">support vectors:</font> datapoints the margin pushes up</li>
</ul>
</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai502.png" style="zoom:50%"></p>
<ul>
<li>decision boundary: $&lt;\mathbf{w}, \mathbf{x}&gt; +b = 0$</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai503.png"></p>
<ul>
<li>Kernel SVM: for non-linearlity<ul>
<li>RBF</li>
<li>Polynomial</li>
<li>Sigmoid</li>
</ul>
</li>
<li>Soft Margin SVM<ul>
<li>Even with kernel trick, it is hardly to guarantee that the training data are linearly separable, thus a soft margin rather than hard margin is used in practice.</li>
</ul>
</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai504.png"></p>
<h2 id="Artificial-Neural-Networks"><a href="#Artificial-Neural-Networks" class="headerlink" title="Artificial Neural Networks"></a>Artificial Neural Networks</h2><ul>
<li>A highly nonlinear function that mimic the structure of biological NN.</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai505.png"></p>
<h3 id="Training-NN"><a href="#Training-NN" class="headerlink" title="Training NN"></a>Training NN</h3><ul>
<li>Optimize weights to minimize the Loss function</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>J(w)=\frac{1}{2} \sum_{k=1}^{c}(y_k-z_k)^2=\frac{1}{2} ||\mathbf{y}-\mathbf{z} ||^{2}<br>$</p>

</blockquote>
<ul>
<li>Training algorithm: gradient descent, with Back Propagation(BP) algorithm as a representative example.</li>
</ul>
<p><strong>BP</strong></p>
<ul>
<li>Update weights between output and hidden layers</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>\nabla w_{ji}=-\eta\frac{\partial{J}}{ \partial{w_{ji}}}<br>$</p>

</blockquote>
<ul>
<li>Update weights between input and hidden layers</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>\frac{\partial{J}}{ \partial{w_{ki}}}=\frac{\partial{J}}{ \partial{net_{k}}} \cdot \frac{ \partial{net_{k}}}{ \partial{w_{ki}}} = -\delta_{k}\frac{ \partial{net_{k}}}{ \partial{w_{ki}}}<br>$</p>

</blockquote>
<ul>
<li>Apply in BP algorithm<ul>
<li>initial $D=\{ (\mathbf{x_1},y_1), \cdots, (\mathbf{x_m}, y_m) \}$ and learning rate $\eta$</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> Optimality conditions are <span class="keyword">not</span> satisfied:</span><br><span class="line">  <span class="keyword">for</span> (x, y) <span class="keyword">in</span> D:</span><br><span class="line">    calc current output by current param</span><br><span class="line">    calc grad <span class="keyword">for</span> output-layer neurons</span><br><span class="line">    calc grad <span class="keyword">for</span> hidden-layer neurons</span><br><span class="line">    update weight <span class="keyword">and</span> thresholds</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h3 id="Some-issues"><a href="#Some-issues" class="headerlink" title="Some issues"></a>Some issues</h3><ul>
<li>Universal Approximation Theory</li>
<li>Fully connected NN (MLP) with more than 1 hidden layer is very difficult to train</li>
<li>etc.</li>
</ul>
<h2 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h2><ul>
<li>A natural way to handle nonmetric data (also applicable to real-valued data).</li>
<li>Tree searching metrics<ul>
<li>Entropy: $i(N) = -\sum_{i}p(w_i)\log{p(w_i)}$</li>
<li>Variance: $i(N) = 1 - \sum_{i}p^2(w_i)$</li>
<li>Misclassification rate: $i(N) = 1 - \max_{i}p(w_i)$</li>
</ul>
</li>
</ul>
<blockquote>
<p>How to contruct a DT ?</p>
</blockquote>
<ol>
<li>Start from the root, keep searching for a rule to branch a node.</li>
<li>At each node, select the rule that leads to the most significant decrease in <strong>impurity</strong> (similar to gradient descent).<ul>
<li>$\Delta i(N) = i(N) - p_L i(N_L) - (1-p_L)i(N_R)$ </li>
</ul>
</li>
<li>When the process terminates, assign class label to the leaf nodes. <ul>
<li>label a leaf node with the label of majority instances that fall into it.</li>
</ul>
</li>
</ol>
<blockquote>
<p>How to control the complexity ?</p>
</blockquote>
<ul>
<li>Setting the maximum height of the tree (early stopping)</li>
<li>Introduce the tree height (or any other complexity measure as a penalty)</li>
<li>Fully grow the tree first, and then prune it (post processing)</li>
</ul>
<h1 id="Lecture-6-Performance-Evaluation-for-Machine-Learning"><a href="#Lecture-6-Performance-Evaluation-for-Machine-Learning" class="headerlink" title="Lecture 6. Performance Evaluation for Machine Learning"></a>Lecture 6. Performance Evaluation for Machine Learning</h1><h2 id="Outline-5"><a href="#Outline-5" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li>Brief view</li>
<li>Performance Metrics</li>
<li>Estimating the Generalization</li>
</ul>
<h2 id="Brief-view"><a href="#Brief-view" class="headerlink" title="Brief view"></a>Brief view</h2><ul>
<li>Be careful when choosing your objective function, two principles:<ul>
<li>Consistent with the user requirements?</li>
<li>Existing easy-to-use algorithm to optimize it (to train the model)?</li>
</ul>
</li>
<li>Do internal tests as much as possible<ul>
<li>estimate the generalization performance as accurate as possible.</li>
</ul>
</li>
<li>Can only reduce rather than remove risk. There is no guarantee in life.</li>
</ul>
<h2 id="Performance-Metrics"><a href="#Performance-Metrics" class="headerlink" title="Performance Metrics"></a>Performance Metrics</h2><blockquote>
<p>Review MA234.</p>
</blockquote>
<ul>
<li>T &amp; F : represents truth of label (标签是否真实)</li>
<li>P &amp; N : represents aspect of label (标签的正反两面)</li>
<li>And there’re 4 cases: TP, TN, FP, FN</li>
<li>Several metrics:<ul>
<li>accuracy 👉 bad when samples are imbalanced</li>
<li>precision</li>
<li>Recall</li>
<li>F-measure ($F_1$) $=\large\frac{2\times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$</li>
</ul>
</li>
</ul>
<blockquote>
<p>细节请参考大数据导论与实践（二）</p>
</blockquote>
<ul>
<li>ROC 👉 TPR / FPR</li>
<li>AUC : slope of ROC, good PF when $AUC\gt 0.75$</li>
</ul>
<h2 id="Estimating-the-Generalization"><a href="#Estimating-the-Generalization" class="headerlink" title="Estimating the Generalization"></a>Estimating the Generalization</h2><ul>
<li>Generalization performance is a random variable. </li>
<li>Split the data in hand into training and testing subsets.<ul>
<li>Random Split</li>
<li>Cross-validation</li>
<li>Bootstrap</li>
</ul>
</li>
<li>Collecting the test performance for many times, calculate the average and standard deviation. </li>
<li>Do statistical tests (check your textbook on statistics)</li>
</ul>
<h1 id="Lecture-7-Unsupervised-Learning"><a href="#Lecture-7-Unsupervised-Learning" class="headerlink" title="Lecture 7. Unsupervised Learning"></a>Lecture 7. Unsupervised Learning</h1><h2 id="Outline-6"><a href="#Outline-6" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li>Why Unsupervised ?</li>
<li>Clustering</li>
<li>K-Means</li>
<li>Dimensionality Reduction</li>
</ul>
<h2 id="Why-Unsupervised"><a href="#Why-Unsupervised" class="headerlink" title="Why Unsupervised ?"></a>Why Unsupervised ?</h2><ul>
<li>In practice, it might neither be tractable to collect sufficient labelled data</li>
<li>Instead, it is relatively easy to accumulate large amount of unlabeled data.</li>
</ul>
<p><strong>Supervised vs. Unsupervised</strong></p>
<ul>
<li>Share the same key factors, i.e., representation + algorithm + evaluation</li>
<li>For supervised learning, since ground-truth is available for the training data, the evaluation (objective function) can be said as <font color="red">objective</font>.</li>
<li>For unsupervised learning, the evaluation is usually less specific and <font color="red">more subjective</font>.</li>
<li>It is more likely that an unsupervised learning problem is ill-defined and the learning output deviate from our intuition.</li>
</ul>
<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><ul>
<li><strong>Def.</strong> a typical ill-defined problem as there is no unique definition of the similarity between clusters.</li>
<li>Idea: gathering similar data into one class<ul>
<li>e.g. Objective Function (to minimize distance)</li>
</ul>
</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>\begin{array}{}<br>J= \underset{i=1}{\overset{k}{\sum}} \underset{x \in D_i }{\sum} || \mathbf{x} - \mathbf{m_i} ||^2 \\<br>i.e. J = \frac{1}{2} \underset{i=1}{\overset{k}{\sum}} n_i \underset{x,x’ \in D_i }{\sum} || \mathbf{x}-\mathbf{x’} ||^2<br>\end{array}<br>$</p>

</blockquote>
<p><strong>Naive Approach</strong></p>
<ul>
<li><font color="dodgerblue">Top-down:</font> following the decision tree idea to split the data recursively. </li>
<li><font color="dodgerblue">Bottom-up:</font> recursively put two instances (or “meta-instances”) into the same group</li>
<li>Basically you need to define <font color="red">similarity metric</font> (e.g., Euclidean distance) first.</li>
</ul>
<blockquote>
<p>层次聚类？</p>
</blockquote>
<h2 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h2><h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><ul>
<li>Given a predefined $K$<ol>
<li>Randomly initialize $K$ cluster centers</li>
<li>Assign each instance to the nearest center</li>
<li>Update the each center as the mean of all the instances in the cluster</li>
<li>Repeat Step 1-3 until the centers do not change any more</li>
</ol>
</li>
</ul>
<blockquote>
<p>Not only similarity metric, but also needs calculating of the average.</p>
</blockquote>
<h3 id="Application-Clustering-for-Graph-Data"><a href="#Application-Clustering-for-Graph-Data" class="headerlink" title="Application: Clustering for Graph Data"></a>Application: Clustering for Graph Data</h3><p><img src="/2025/01/01/Artificial-Intelligence/ai701.png"></p>
<h2 id="Dimensionality-Reduction"><a href="#Dimensionality-Reduction" class="headerlink" title="Dimensionality Reduction"></a>Dimensionality Reduction</h2><h3 id="Principle-Component-Analysis"><a href="#Principle-Component-Analysis" class="headerlink" title="Principle Component Analysis"></a>Principle Component Analysis</h3><ul>
<li>Given a n-by-d data set, can we map it into a lower dimensional space with a <font color="red">linear</font> transformation, while only introduce the minimum information loss?</li>
<li>Suppose we want to reduce data dimension from $n$ to $k$<ol>
<li>Init a dataset: $X=\{x_1, x_2, \cdots , x_m\}$</li>
<li>Calculate the mean value and minus it (decentralize)</li>
<li>Calculate the covariance matrix by $C= \frac{1}{n}XX^T$</li>
<li>Calculate the eigenvalues and corresponding eigenvectors</li>
<li>Sort the eigenvectors by eigenvalues (large to small) and select the top $k$ as eigenvector matrix $P$</li>
<li>$Y=PX$ to get new data.</li>
</ol>
</li>
</ul>
<h3 id="Locally-Linear-Embedding"><a href="#Locally-Linear-Embedding" class="headerlink" title="Locally Linear Embedding"></a>Locally Linear Embedding</h3><p><img src="/2025/01/01/Artificial-Intelligence/ai702.png"></p>
<ul>
<li>Idea flow:<ol>
<li>Identify nearest neighbors for each instance</li>
<li>Calculate the linear weights for each instances to be reconstructed by its neighbors<ul>
<li>$\varepsilon(W)=\underset{i}{\sum}|X_i-\underset{j}{\sum} W_{ij} X_j|^2$</li>
</ul>
</li>
<li>Use W as the local structure information to be preserved (i.e., fix $W$), find the optimal values (say $Y$) for $X$ in the lower dimensional space.<ul>
<li>$\Phi (Y)=\underset{i}{\sum} |Y_i - \underset{j}{\sum} W_{ij} Y_j|^2$</li>
</ul>
</li>
</ol>
</li>
</ul>
<h1 id="Lecture-8-Recommender-System"><a href="#Lecture-8-Recommender-System" class="headerlink" title="Lecture 8. Recommender System"></a>Lecture 8. Recommender System</h1><h2 id="Outline-7"><a href="#Outline-7" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li>Overview of recommender system (RS)</li>
<li>How does RS do recommendation?</li>
<li>How to build a RS?</li>
</ul>
<h2 id="Overview-of-recommender-system-RS"><a href="#Overview-of-recommender-system-RS" class="headerlink" title="Overview of recommender system (RS)"></a>Overview of recommender system (RS)</h2><ul>
<li>Recommender System recommend new items to its user.</li>
<li>Based on ?<ul>
<li>The items that the user has been interacted. [根据相似物品]</li>
<li>The users who have been interacted with same items which this user also been interacted. [根据相似用户]</li>
</ul>
</li>
<li>The recommendation is <font color="red">personalized</font>. </li>
<li>The key of Recommender System is a <font color="red">score function</font>.<ul>
<li>Input: a user and an item.</li>
<li>Return value: a score, indicating how likely the user would be interested in the item.</li>
</ul>
</li>
</ul>
<h2 id="How-does-RS-do-recommendation"><a href="#How-does-RS-do-recommendation" class="headerlink" title="How does RS do recommendation?"></a>How does RS do recommendation?</h2><p><img src="/2025/01/01/Artificial-Intelligence/ai801.png"></p>
<ul>
<li>RS basically estimate the probability of interaction between a user and an item, </li>
<li>The score function is essentially a <fotn color="red">model trained with data&lt;/font&gt;.</fotn></li>
<li>In practice, the score function could be very complicated since<ul>
<li>The RS needs to be efficient (make recommendations in seconds)</li>
<li>In many applications, we may have millions of users and items</li>
<li>There is always a trade-off between efficiency and accuracy</li>
</ul>
</li>
</ul>
<h2 id="How-to-build-a-RS"><a href="#How-to-build-a-RS" class="headerlink" title="How to build a RS?"></a>How to build a RS?</h2><h3 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h3><ul>
<li><strong>Input:</strong> Historical user-item interaction records or additional side information (e.g. user’s social relations, item’s knowledge, etc.)</li>
<li><strong>Output:</strong> The score function</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai802.png"></p>
<h3 id="Typical-methods"><a href="#Typical-methods" class="headerlink" title="Typical methods"></a>Typical methods</h3><ul>
<li><strong>Content-based method</strong>:<ul>
<li>The very basic idea: build a regression/classification model for each user<ul>
<li>Focusing on the side information of the items (i.e., attributes, features of items)</li>
<li>Suggesting items by comparing their features to a user’s past behaviors</li>
</ul>
</li>
</ul>
</li>
<li><strong>Collaborative Filtering method</strong>: <ul>
<li>Predicting user preferences based on the behaviors of other users. </li>
<li>Based on the historical user-item interaction data </li>
</ul>
</li>
<li><strong>Hybrid method</strong>: Combination of CF-based and Content-based method</li>
</ul>
<h3 id="CF-based-method"><a href="#CF-based-method" class="headerlink" title="CF-based method"></a>CF-based method</h3><ul>
<li>Attributes/features of users and items are not available, <ul>
<li>How to build the regression/classification model (as the score function)?</li>
<li>Learning representation of users and items</li>
</ul>
</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai803.png"></p>
<h3 id="Represented-by-correlation"><a href="#Represented-by-correlation" class="headerlink" title="Represented by correlation"></a>Represented by correlation</h3><ul>
<li>Represent the user/item by its correlation with the other users/items.<ul>
<li>Users with similar historical interactions are likely to have the same preferences.</li>
<li>Items that are interacted by similar users are likely to have hidden commonalities (共性).</li>
</ul>
</li>
<li>A user/item is represented by a vector that consists of all the correlation between itself and all the users/items.</li>
</ul>
<blockquote>
<font color="red">Q: How to define the correlation between 2 users or items?</font>
<br>
<font color="green">A: Pearson Correlation Coefficient, a normalized measurement of the covariance</font>

</blockquote>
<blockquote class="blockquote-center">
<p>$<br>c_{u_1u_2}=\large\frac{\underset{i\in M}{\sum} (r_{u_1,i} - \bar{r}_{u_1}) (r_{u_2,i} - \bar{r}_{u_2}) }{\sqrt{\underset{i\in M}{\sum} (r_{u_1,i} - \bar{r}_{u_1})^2} \sqrt{\underset{i\in M}{\sum} (r_{u_2,i} - \bar{r}_{u_2})^2}}<br>$</p>

</blockquote>
<ul>
<li>An example of user’s Pearson Correlation Coefficient</li>
<li>$M$: The item set</li>
<li>$r_{u,i}$: Interaction record between user $u$ and item $i$</li>
<li>$\bar{r}_u$: Mean value of all the interaction records of user $u$</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai804.png"></p>
<ul>
<li><strong>Advantage</strong>: high interpretability<ul>
<li>It is easy to explain why the system recommend the item to the user.</li>
</ul>
</li>
<li><strong>Disadvantage</strong>: low scalability<ul>
<li>What if there are millions of users and millions of items?</li>
<li>High-dimensional, sparse feature representation</li>
</ul>
</li>
</ul>
<h3 id="Represent-by-matrix-factorization"><a href="#Represent-by-matrix-factorization" class="headerlink" title="Represent by matrix factorization"></a>Represent by matrix factorization</h3><ul>
<li>A matrix $R\in \mathbb{R}^{n\times m}$ approximate to the product of two matrix:</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>R\approx PQ^T ,\ P\in \mathbb{R}^{n\times d},\ Q\in \mathbb{R}^{m\times d}<br>$</p>

</blockquote>
<ul>
<li>Representing the user and item as a $d$-dimension vector</li>
<li>Matrix $P$, $Q$ consist of the representation vectors of all the users and items.</li>
<li>The low-dimension vector representation is also called as <strong>embedding vector</strong>.</li>
</ul>
<blockquote class="blockquote-center">
<p>$<br>\underset{P,Q}{\min} \underset{r_{u,i} \in R’}{\sum} ||r_{u,i} - r’_{u,i} ||,\ \text{where } r’_{u,i} = P_u Q_i^T<br>$</p>

</blockquote>
<ul>
<li>In this “objective function”, $P_u$ is user $u$’s embedding vector, $Q_i$ is item $i$’s embedding vector<ul>
<li>$r’_{u,i} = f(P_u, Q_i) = P_uQ_i^T$ is a simple example for this function.</li>
</ul>
</li>
<li>If we replace the matrix multiplication with a complex model $M$, such as MLP<ul>
<li>objective function will be : $\large\underset{P,Q,M}{\min} \underset{r_{u,i}\in R’}{\sum} ||r_{u,i}-f(P_u, Q_i, M)||$</li>
<li>The model with higher complexity may have better prediction performance in <strong>big data scenario</strong> (as an optimization)</li>
</ul>
</li>
</ul>
<h1 id="Lecture-9-Automated-Machine-Learning"><a href="#Lecture-9-Automated-Machine-Learning" class="headerlink" title="Lecture 9. Automated Machine Learning"></a>Lecture 9. Automated Machine Learning</h1><h2 id="Tuning-Hyper-parameters"><a href="#Tuning-Hyper-parameters" class="headerlink" title="Tuning Hyper-parameters"></a>Tuning Hyper-parameters</h2><blockquote>
<p>How to tune the hyper-parameters?</p>
</blockquote>
<p><img src="/2025/01/01/Artificial-Intelligence/ai901.png"></p>
<ul>
<li>Grid Search<ul>
<li>Too costly</li>
</ul>
</li>
<li>More efficient ways?<ul>
<li>Use heuristic Search (e.g., using Black-Box optimization algorithms)</li>
<li>Sometimes, good surrogate of generalization is available to accelerate the evaluation</li>
</ul>
</li>
</ul>
<blockquote>
<p>Too short ? Sorry, my lecture slides is only 6 pages.</p>
</blockquote>
<h1 id="Lecture-10-Logical-Agents"><a href="#Lecture-10-Logical-Agents" class="headerlink" title="Lecture 10. Logical Agents"></a>Lecture 10. Logical Agents</h1><h2 id="Outline-8"><a href="#Outline-8" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li>Knowledge-based Agents</li>
<li>Represent Knowledge with Logic</li>
<li>(Propositional) Logic</li>
<li>Inference with Propositional Logic</li>
</ul>
<h2 id="Knowledge-based-Agents"><a href="#Knowledge-based-Agents" class="headerlink" title="Knowledge-based Agents"></a>Knowledge-based Agents</h2><p><strong>Agent Components</strong></p>
<ul>
<li>Intelligent agents need <font color="red">knowledge</font> about the world to choose good actions/decisions.</li>
<li>Knowledge = {sentences} in a knowledge representation language (formal language).</li>
<li>A sentence is an assertion about the world.</li>
<li>A knowledge-based agent is composed of:<ol>
<li><font color="dodgerblue">Knowledge base</font>: domain-specific content.</li>
<li><font color="dodgerblue">Inference mechanism</font>: domain-independent algorithms.</li>
</ol>
</li>
</ul>
<p><strong>Agent Requirements</strong></p>
<ul>
<li>Represent states, actions, etc.</li>
<li>Incorporate new percepts</li>
<li>Update internal representations of the world</li>
<li>Deduce hidden properties of the world</li>
<li>Deduce appropriate actions</li>
</ul>
<p><strong>Declarative approach to building an agent</strong></p>
<ul>
<li>Add new sentences: <em>Tell</em> it what it needs to know</li>
<li>Query what is known: <em>Ask</em> itself what to do - answers should follow from the KB</li>
</ul>
<blockquote>
<p>Use a game as an example:</p>
</blockquote>
<p><img src="/2025/01/01/Artificial-Intelligence/ai1001.png" style="zoom:60%"></p>
<ul>
<li>Actuators:<ul>
<li>Left turn, Right turn, Forward, Grab, Release, Shoot </li>
</ul>
</li>
<li>Sensors:<ul>
<li>Stench, Breeze, Glitter, Bump, Scream</li>
<li>Represented as a 5-element list</li>
<li>Example: [Stench, Breeze, None, None, None]</li>
</ul>
</li>
</ul>
<h2 id="Represent-Knowledge-with-Logic"><a href="#Represent-Knowledge-with-Logic" class="headerlink" title="Represent Knowledge with Logic"></a>Represent Knowledge with Logic</h2><ul>
<li><font color="dodgerblue">Knowledge base</font>: a set of sentences in a formal representation</li>
<li><font color="dodgerblue">Syntax</font>: defines well-formed sentences in the language</li>
<li><font color="dodgerblue">Semantic</font>: defines the truth or meaning of sentences in a world</li>
<li><font color="dodgerblue">Inference</font>: a procedure to derive a new sentence from other ones.</li>
</ul>
<table>
<tr>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1002.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1003.png"></td>
</tr>
</table>

<h3 id="Inference-1"><a href="#Inference-1" class="headerlink" title="Inference"></a>Inference</h3><ul>
<li>Inference: the procedure of deriving a sentence from another sentence</li>
<li><font color="red">Model Checking</font>: A basic (and general) idea to inference</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai1004.png" style="zoom:50%"></p>
<h2 id="Propositional-Logic"><a href="#Propositional-Logic" class="headerlink" title="Propositional Logic"></a>Propositional Logic</h2><blockquote>
<p>Most have been learnt in CS201 Discrete Mathemetics</p>
</blockquote>
<p><strong>Def.</strong> A proposition is a declarative statement that’s either True or False.</p>
<ul>
<li><font color="green">Recall:</font><ul>
<li>Negation</li>
<li>AND</li>
<li>OR</li>
<li>Implication</li>
<li>Biconditional, etc.</li>
</ul>
</li>
</ul>
<h2 id="Inference-with-Propositional-Logic"><a href="#Inference-with-Propositional-Logic" class="headerlink" title="Inference with Propositional Logic"></a>Inference with Propositional Logic</h2><ul>
<li>Our inference algorithm target:</li>
<li><strong>Sound</strong>: oes not infer false formulas, that is, derives only entailed sentences.<ul>
<li>$\{ \alpha | KB \vdash \alpha \} \subseteq \{ \alpha | KB \models \alpha \}$</li>
</ul>
</li>
<li><strong>Complete</strong>: derives ALL entailed sentences.<ul>
<li>$\{ \alpha | KB \vdash \alpha \} \supseteq \{ \alpha | KB \models \alpha \}$</li>
</ul>
</li>
<li>That is, we want a <strong>Logical Equivalent</strong>: $p\equiv q$</li>
</ul>
<blockquote>
<p>Go review some Inferences instance:</p>
<p>e.g. Modus Ponens, Modus Tollens, etc.</p>
</blockquote>
<h3 id="Inference-as-a-search-problem"><a href="#Inference-as-a-search-problem" class="headerlink" title="Inference as a search problem"></a>Inference as a search problem</h3><ul>
<li><font color="red">Initial state:</font> The initial KB</li>
<li><font color="red">Actions:</font> all inference rules applied to all sentences that match the top of the inference rule</li>
<li><font color="red">Results:</font> add the sentence in the bottom half of the inference rule</li>
<li><font color="red">Goal:</font> a state containing the sentence we are trying to prove.</li>
<li><font color="dodgerblue">Completeness Issue:</font> if the inference rules to use are not sufficient, the goal can not be obtained.</li>
</ul>
<blockquote>
<p>How to ensure soundness ?</p>
</blockquote>
<ul>
<li>The <strong>idea of inference</strong> is to repeat applying inference rules to the KB.</li>
<li>Inference can be applied whenever suitable premises are found in the KB.</li>
</ul>
<blockquote>
<p>What aboud completeness ?</p>
</blockquote>
<ul>
<li>Two ways to ensure completeness:<ul>
<li><strong>Proof by resolution</strong>: use powerful inference rules (resolution rule)</li>
<li><strong>Forward or Backward chaining</strong>: use of modus ponens on a restricted form of propositions (Horn clauses)</li>
</ul>
</li>
</ul>
<h3 id="Proof-by-resolution"><a href="#Proof-by-resolution" class="headerlink" title="Proof by resolution"></a>Proof by resolution</h3><p><img src="/2025/01/01/Artificial-Intelligence/ai1005.png" style="zoom:60%"></p>
<ul>
<li>Two cases to end loop:<ul>
<li>there are no new clauses that can be added, in which case $KB$ doesn’t ential $\alpha$; or,</li>
<li>two clauses resolve to yield the empty clause, in which case $KB$ entails $\alpha$</li>
</ul>
</li>
</ul>
<h3 id="Forward-chaining"><a href="#Forward-chaining" class="headerlink" title="Forward chaining"></a>Forward chaining</h3><p><img src="/2025/01/01/Artificial-Intelligence/ai1006.png" style="zoom:60%"></p>
<ul>
<li><strong>Idea</strong>: Find any rule whose premises are satisfied in the $KB$, add its conclusion to the KB, until query is found</li>
</ul>
<table>
<tr>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1007.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1008.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1009.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1010.png"></td>
</tr>
<tr>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1011.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1012.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1013.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1014.png"></td>
</tr>
</table>

<h3 id="Backward-chaining"><a href="#Backward-chaining" class="headerlink" title="Backward chaining"></a>Backward chaining</h3><ul>
<li><strong>Idea:</strong> Works backwards from the query $q$</li>
<li>To prove $q$ by Backward Chaining:<ul>
<li>Check if $q$ is known already, or</li>
<li>Prove by Backward Chaining all premises of some rule concluding $q$.</li>
</ul>
</li>
<li>Avoid loops: check if new subgoal is already on the goal stack</li>
<li>Avoid repeated work: check if new subgoal<ul>
<li>has already been proved true, or</li>
<li>has already failed</li>
</ul>
</li>
</ul>
<table>
<tr>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1015.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1016.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1017.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1018.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1019.png"></td>
</tr>
<tr>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1020.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1021.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1022.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1023.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1024.png"></td>
</tr>
</table>

<ul>
<li>Explanation:<ul>
<li>start from query $Q$, and $A$, $B$ has been known.</li>
<li>check recursively each node that “should be proved right”</li>
<li>for one node:<ul>
<li>if its “successors” wait to be proved (e.g. L 👈 <strong>P</strong>, A)</li>
<li>or it’s unknown (neither “green” nor “red”), then avoid it</li>
</ul>
</li>
<li>else this node is proved (turn “red”)</li>
</ul>
</li>
<li>Suppose: B is unknown<ul>
<li>then step 5 (want to prove L 👈 A, <strong>B</strong>) failed</li>
<li>and L cannot be proved, and query $Q$ failed immediately too.</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>Forward vs. Backward</strong></p>
</blockquote>
<ul>
<li>Forward chaining:<ul>
<li>Data-driven, automatic, unconscious processing,</li>
<li>May do lots of work that is irrelevant to the goal</li>
</ul>
</li>
<li>Backward chaining:<ul>
<li>Goal-driven, appropriate for problem-solving,</li>
<li>Complexity of BC can be much less than linear in size of KB</li>
</ul>
</li>
</ul>
<h3 id="DPLL"><a href="#DPLL" class="headerlink" title="DPLL"></a>DPLL</h3><blockquote>
<p>The <strong>DPLL algorithm</strong> is similar to <strong>Backtracking</strong> for CSP, but using various problem dependent information/heuristics, such as Early Termination, Pure symbol heuristic and Unit clause heuristic.</p>
</blockquote>
<p><img src="/2025/01/01/Artificial-Intelligence/ai1025.png" style="zoom:60%"></p>
<blockquote>
<p>概念介绍：如下的式子被称为合取范式（CNF），式子中只包含逻辑与，逻辑或和逻辑非，且每个部分由<strong>逻辑与</strong>连接</p>
<p>$(a\vee b\vee \neg c)\wedge \cdots \wedge (a\vee d \vee \neg d)$</p>
<ul>
<li>括号部分为该公式的<strong>子句(clause)</strong>，每个子句中的变量或变量的否定为<strong>文字(literal/symbol)</strong></li>
<li>要使整个公式为 True，则每个子句都必须为 True，也就是说，每个子句中至少有一个文字为 True</li>
<li>DPLL 算法简化步骤实际上就是<font color="hotpink">移除所有在赋值后值为 True 的子句，以及所有在赋值后值为 False 的文字</font>。<ul>
<li>简化步骤分两步：孤立文字消去（Pure Symbol）和单位子句传播（Unit Clause）</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li>In <strong>pure symbol elimination</strong>, we try to find <font color="red">symbols</font> that only appear <font color="red">once in all clauses</font>.<ul>
<li>If the symbol is in form $a$ (positive), then it’s assigned to <font color="limegreen">True</font> ;</li>
<li>If the symbol is in form $\neg a$ (negative), then it’s assigned to <font color="orangered">False</font> ;</li>
<li>then we check the clause containing this symbol if it’s true or false. <em>(try to eliminate)</em></li>
</ul>
</li>
<li>In <strong>unit clause propagation</strong>, we try to find <font color="red">clauses with only one literal</font>, or <font color="red">clauses with one literal that is unknown</font> (and it must cause the whole clause unknown).<ul>
<li>E.g. $(a\vee b\vee c\vee \neg d)\wedge (\neg a\vee c)\wedge (\neg c\vee d)\wedge (a)$<ul>
<li>find $a$ as unit clause, then we say $a$ must be <font color="limegreen">True</font>, and reduce the sentence $\to (c)\wedge (\neg c\vee d)\wedge (a)$ ;</li>
<li>then find $c$ as unit clause, same process $\to (c)\wedge(d)\wedge(a)$ ;</li>
</ul>
</li>
</ul>
</li>
<li>At last, we got a <code>model</code> with some assigned literal. That’s the solution we want.</li>
</ul>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li><strong>Inference with Propositional Logic</strong> 👉 we want an inference algorithm that is:<ul>
<li>sound (does not infer false formulas), and</li>
<li>ideally, complete too (derives all true formulas).</li>
</ul>
</li>
<li>Limits ?<ul>
<li>PL is not expressive enough to describe all the world around us. It can’t express information about different object and the relation between objects.</li>
<li>PL is not compact. It can’t express a fact for a set of objects without enumerating all of them which is sometimes <strong>impossible</strong>.</li>
</ul>
</li>
</ul>
<blockquote>
<p>Review: First Order Logic (learnt in Discrete Mathematics)</p>
<ul>
<li>Concept of Syntax, Semantics, Entailment(necessary truth of one sentence given another), etc.</li>
<li><p>Forward, backward chaining are linear in time, complete for horn clauses. Resolution is complete for propositional logic.</p>
</li>
<li><p>Pros</p>
<ul>
<li>Intelligibility of models: models are encoded explicitly</li>
</ul>
</li>
<li>Cons<ul>
<li>Do not handle uncertainty</li>
<li>Rule-based and do not use data (Machine Learning)</li>
<li>It is hard to model every aspect of the world</li>
</ul>
</li>
</ul>
</blockquote>
<h1 id="Lecture-11-First-Order-Logic"><a href="#Lecture-11-First-Order-Logic" class="headerlink" title="Lecture 11. First Order Logic"></a>Lecture 11. First Order Logic</h1><h2 id="Inference-with-FOL"><a href="#Inference-with-FOL" class="headerlink" title="Inference with FOL"></a>Inference with FOL</h2><blockquote>
<p><strong>Basic concept of FOL</strong></p>
<p>Three basic component: Objects, Relations, Functions</p>
</blockquote>
<p><img src="/2025/01/01/Artificial-Intelligence/ai1101.png" style="zoom:60%"></p>
<ul>
<li><font color="green">Recall</font>:<ul>
<li>Universal Instantiation, Existential Instantiation, etc.</li>
<li>Reduce FOL to simple format</li>
</ul>
</li>
<li>Better Ideas to Inference with FOL: Unification<ul>
<li>Resolution</li>
<li>Chaining Algorithms (In lec 10.)</li>
</ul>
</li>
</ul>
<h1 id="Lecture-12-Representing-and-Inference-with-Uncertainty"><a href="#Lecture-12-Representing-and-Inference-with-Uncertainty" class="headerlink" title="Lecture 12. Representing and Inference with Uncertainty"></a>Lecture 12. Representing and Inference with Uncertainty</h1><h2 id="Outline-9"><a href="#Outline-9" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li>Uncertainty and Rational Decisions</li>
</ul>
<h2 id="Uncertainty-and-Rational-Decisions"><a href="#Uncertainty-and-Rational-Decisions" class="headerlink" title="Uncertainty and Rational Decisions"></a>Uncertainty and Rational Decisions</h2><ul>
<li>Alternative to Logic<ul>
<li>Utility theory: Assign utility to each state/actions</li>
<li>Probability theory: Summarize the uncertainty associated with each state</li>
<li>Rational Decisions: Maximize the expected utility (Probability + Utility) </li>
<li>Thus we need to represent states in the language of probability</li>
</ul>
</li>
<li><font color="red">In a word, use probability to replace logic.</font>



</li>
</ul>
<h2 id="Basic-Probability-Theory-and-Usage"><a href="#Basic-Probability-Theory-and-Usage" class="headerlink" title="Basic Probability Theory and Usage"></a>Basic Probability Theory and Usage</h2><table>
<tr>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1201.png"></td>
    <td><img src="/2025/01/01/Artificial-Intelligence/ai1202.png"></td>
</tr>
</table>

<blockquote>
<p><font color="green">Recall:</font> MA212 概率论与数理统计</p>
<p>贝叶斯公式，条件概率，联合概率等</p>
</blockquote>
<h2 id="Bayesian-Networks"><a href="#Bayesian-Networks" class="headerlink" title="Bayesian Networks"></a>Bayesian Networks</h2><ul>
<li>What is a BN ?<ul>
<li>A Directed Acyclic Graph (DAG).</li>
<li>Each node is a random variable, associated with conditional distribution. </li>
<li>Each arc (link) represent <font color="red">direct influence</font> of a parent node to a child node.</li>
</ul>
</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai1203.png" style="zoom:60%"></p>
<ul>
<li>In the above exmaple, a Conditional Probability Table (CPT) is construct for each node<ul>
<li>Easier to utilize independence and conditional dependence relations to define the joint distribution.</li>
</ul>
</li>
<li>How to construct a <strong>CPT for BN</strong>?</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai1204.png" style="zoom:60%"></p>
<h3 id="Inference-with-BN"><a href="#Inference-with-BN" class="headerlink" title="Inference with BN"></a>Inference with BN</h3><ul>
<li>Given a Bayesian Network, and an (or some) observed events, which specifies the value for <font color="red">evidence variables</font>, we want to know the probability distribution of one (or several) <font color="red">query variables</font> $\color{red}X$, i.e. $P(X | \text{events})$</li>
<li>First we try enumeration (calc all possible cases)</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai1205.png" style="zoom:70%"></p>
<ul>
<li>and it’s time consuming.</li>
<li>A way to simplify: Enumeration by Variable Elimination</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai1206.png" style="zoom:70%"></p>
<h3 id="Approximate-Inference-with-BN"><a href="#Approximate-Inference-with-BN" class="headerlink" title="Approximate Inference with BN"></a>Approximate Inference with BN</h3><ul>
<li>Basic Idea:<ol>
<li>Draw N samples from a sampling distribution $S$</li>
<li>Compute an approximate posterior probability (后验概率) $\hat{P}$</li>
<li>Show this converge to the true probability $P$</li>
</ol>
</li>
<li>Outline<ul>
<li>Sampling from an empty network</li>
<li>Rejection sampling: reject samples disagreeing with evidence</li>
<li>Likelihood weighting: use evidenve to weight samples</li>
<li>Markov Chain Monte Carlo (MCMC): sample from a stochastic process (随机过程) whose stationary distribution is the true posterior.</li>
</ul>
</li>
</ul>
<h4 id="Sampling-from-an-empty-network"><a href="#Sampling-from-an-empty-network" class="headerlink" title="Sampling from an empty network"></a>Sampling from an empty network</h4><p><img src="/2025/01/01/Artificial-Intelligence/ai1207.png" style="zoom:60%"></p>
<h4 id="Rejection-Sampling"><a href="#Rejection-Sampling" class="headerlink" title="Rejection Sampling"></a>Rejection Sampling</h4><p><img src="/2025/01/01/Artificial-Intelligence/ai1208.png" style="zoom:80%"></p>
<h4 id="Likelihood-Weighting"><a href="#Likelihood-Weighting" class="headerlink" title="Likelihood Weighting"></a>Likelihood Weighting</h4><p><img src="/2025/01/01/Artificial-Intelligence/ai1209.png" style="zoom:80%"></p>
<h4 id="MCMC"><a href="#MCMC" class="headerlink" title="MCMC"></a>MCMC</h4><p><img src="/2025/01/01/Artificial-Intelligence/ai1210.png" style="zoom:70%"></p>
<h3 id="How-to-construct-a-BN-or-KB-in-general"><a href="#How-to-construct-a-BN-or-KB-in-general" class="headerlink" title="How to construct a BN (or KB in general) ?"></a>How to construct a BN (or KB in general) ?</h3><ul>
<li>Challenge<ul>
<li>Big Data</li>
</ul>
</li>
<li>Methods<ul>
<li>Structural Learning</li>
<li>Parameter Estimation</li>
</ul>
</li>
<li>Similar to Neural Networks<ul>
<li>Structural Learning: Identify the network structure</li>
<li>Parameter Estimation: find VALUEs for parameters associated with an edge<ul>
<li>Depending on how you define the relationship between events/nodes<ul>
<li>values in a CPT</li>
<li>parameters of a probability density function</li>
</ul>
</li>
</ul>
</li>
<li>A machine learning or search problem again.</li>
</ul>
</li>
</ul>
<h1 id="Lecture-13-Knowledge-Graph"><a href="#Lecture-13-Knowledge-Graph" class="headerlink" title="Lecture 13. Knowledge Graph"></a>Lecture 13. Knowledge Graph</h1><h2 id="Overview-of-Knowledge-Graph-KG"><a href="#Overview-of-Knowledge-Graph-KG" class="headerlink" title="Overview of Knowledge Graph (KG)"></a>Overview of Knowledge Graph (KG)</h2><blockquote>
<p>What is Knowledge Graph?</p>
</blockquote>
<ul>
<li>To make a knowledge base (KB) of <strong>practical significance</strong>, we need to:<ul>
<li>Set a proper boundary for “knowledge”, which means:<ul>
<li>bound the scope of the KB (and thus its representation)</li>
<li>bound the utility (application) of the KB</li>
</ul>
</li>
</ul>
</li>
<li>The idea of KG stems from <strong>Semantic Network</strong>.<ul>
<li>Knowledge Graph: Large-scale semantic network</li>
</ul>
</li>
<li>SN/KG uses vertexes and edges to represent knowledge graphically.<ul>
<li><strong>Vertexes</strong>: entities and concepts</li>
<li><strong>Edges</strong>: relations and properties</li>
</ul>
</li>
</ul>
<p><img src="/2025/01/01/Artificial-Intelligence/ai1301.png" style="zoom:90%"></p>
<h2 id="How-to-construct-Knowledge-Graph？"><a href="#How-to-construct-Knowledge-Graph？" class="headerlink" title="How to construct Knowledge Graph？"></a>How to construct Knowledge Graph？</h2><ul>
<li>Heterogeneous directed graphs.<ul>
<li>The KG can be represented as a graph $\mathcal{G}=(V,E)$ , $V$ is vertex set (entities set), $E$ is the edge set (relations set).</li>
</ul>
</li>
<li>RDF：Resource Description Framework, an XML Document standard from W3C<ul>
<li>use relation triplet <code>&lt;head entity, relation type, tail entity&gt;</code> to describe a relation.</li>
<li><strong>Head entity</strong>: the subject of this relation</li>
<li><strong>Relation type</strong>: the category of this relation</li>
<li><strong>Tail entity</strong>: the object of this relation</li>
</ul>
</li>
</ul>
<p><strong>The General (Semi-)Automatic Viewpoint</strong></p>
<p><img src="/2025/01/01/Artificial-Intelligence/ai1302.png" style="zoom:60%"></p>
<h3 id="Automatic-Entity-Recognition"><a href="#Automatic-Entity-Recognition" class="headerlink" title="Automatic Entity Recognition"></a>Automatic Entity Recognition</h3><ul>
<li>Identify meaningful entities based on the statistical metrics of vocabulary across various texts.<ul>
<li>Input: Documents (text)</li>
<li>Output: A set of entities</li>
</ul>
</li>
<li><strong>TF-IDF</strong> (Term Frequency–Inverse Document Frequency):<ul>
<li><font color="dodgerblue">Idea:</font> If a word appears frequently in one document but infrequently in others, it is more likely to be a meaningful entity.</li>
<li>For a corpus of documents:<ul>
<li>Term Frequency (TF): $P(w|d)$</li>
<li>Inverse Document Frequency (IDF): $\log{\left(\frac{|D|}{|\{ d\in D|w\in d \}|}\right)}$</li>
<li>TF-IDF: TF $\times$ IDF</li>
</ul>
</li>
</ul>
</li>
<li><strong>Entropy</strong> :<ul>
<li><font color="dodgerblue">Idea:</font> If a word has a rich variety of <font color="red">neighboring words</font>, it is likely be a meaningful entity<ul>
<li>$H(u) = - \sum_{x\in \mathcal{X}} p(x) \log{p(x)}$</li>
<li>$p(x)$ is the probability of a certain left neighbor (right neighbor) word, $\mathcal{X}$ is the set of all left neighbor (right neighbor) characters of $u$.</li>
<li>The larger $H(u)$ is, more abundant the set of $u$’s neighbors is.</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>Also, some ML techniques can solve the NER(Name Entity Recognition) tasks. Considering <strong>input</strong> is a sentence, <strong>output</strong> is the label of each word in the sentence.</p>
</blockquote>
<p><img src="/2025/01/01/Artificial-Intelligence/ai1303.png" style="zoom:80%"></p>
<h3 id="Automatic-Relation-Extraction"><a href="#Automatic-Relation-Extraction" class="headerlink" title="Automatic Relation Extraction"></a>Automatic Relation Extraction</h3><blockquote>
<p>By recognizing entities, now AI should “learn” about relations.</p>
</blockquote>
<ul>
<li>Using machine learning techniques, model the Relation Extraction process as a Text Classification Problem.<ul>
<li>It’s also a supervised learning task.</li>
</ul>
</li>
<li>Input is a sentence that contains 2 entities. Output is the category of the relation that the sentence express.<ul>
<li>Input: <font color="red">Zihan Zhang</font> will join the <font color="green">ICPC</font>.</li>
<li>Output: participate in</li>
</ul>
</li>
<li><strong>Relation extraction task</strong> can be solved by the following technologies:<ul>
<li>RNN, Transformers, …</li>
</ul>
</li>
</ul>
<h3 id="Knowledge-Graph-Completion"><a href="#Knowledge-Graph-Completion" class="headerlink" title="Knowledge Graph Completion"></a>Knowledge Graph Completion</h3><p><img src="/2025/01/01/Artificial-Intelligence/ai1304.png" style="zoom:80%"></p>
<ul>
<li>2 ways for completion task<ul>
<li>Path-based method</li>
<li>Embedding-based method</li>
</ul>
</li>
</ul>
<ul>
<li>Path-based is interpretable, so we skip it.</li>
<li>Embedding-based methods represent the entities and relation types in the KG as a <strong>low-dimensional real value vector</strong> (also called embedding).<ul>
<li>Design a score function $\mathcal{g}(h,r,t)$. Get suitable embedding for entities and relation types.<ul>
<li>$h$, $r$, and $t$ are embeddings of head entity $h$, relation type $r$, and tail entity $t$ respectively.</li>
<li>Higher $\mathcal{g}(h,r,t)$ means that the relation is more possible to be true.</li>
</ul>
</li>
</ul>
</li>
<li>How to get suitable embedding for entities and relation types?<ul>
<li>Consider: All the relations in the KG should have higher score than any relation that is not in the KG.</li>
<li>Objective Function: $\min \underset{(h,r,t)\in \mathcal{g}}{\sum} \underset{(h’,r’,t’)\notin \mathcal{g}}{\sum} \left[\mathcal{g}(h’,r’,t’) - \mathcal{g}(h,r,t)\right]_{+}$</li>
<li>Get suitable embedding by <strong>gradient descent</strong>.</li>
</ul>
</li>
</ul>
<h2 id="KG-Based-Recommender-System"><a href="#KG-Based-Recommender-System" class="headerlink" title="KG-Based Recommender System"></a>KG-Based Recommender System</h2><p><img src="/2025/01/01/Artificial-Intelligence/ai1305.png" style="zoom:80%"><br><img src="/2025/01/01/Artificial-Intelligence/ai1306.png" style="zoom:80%"><br><img src="/2025/01/01/Artificial-Intelligence/ai1307.png" style="zoom:80%"><br><img src="/2025/01/01/Artificial-Intelligence/ai1308.png" style="zoom:80%"></p>
<ul>
<li>We can get the <strong>feature of the user and item</strong> from the new graph that is mixed by KG and interaction records.</li>
<li>A typical method is GNN (Graph Neural Network):<ul>
<li>There is an initial embedding for each node in the graph.</li>
<li>The final embedding of each node is calculated by the embeddings of its neighborhood.</li>
<li>Result of $f(u,w)$ is calculated according to the final embeddings of user<br>$u$ and item $w$ by a model $M$, such as MLP or matrix multiplication.</li>
</ul>
</li>
</ul>
<h1 id="Review-and-Semester-Summary"><a href="#Review-and-Semester-Summary" class="headerlink" title="Review and Semester Summary"></a>Review and Semester Summary</h1><blockquote>
<p>I can build a knowledge base here to tell you what we’ve learnt in AI course. 😂 </p>
<p>Just a framework.</p>
</blockquote>
<ul>
<li>Problem-solving<ul>
<li>Classical search</li>
<li>Beyondclassical search</li>
<li>Problem-Specific Search</li>
</ul>
</li>
<li>MachineLearning<ul>
<li>Supervised Learning</li>
<li>Performance Evaluation</li>
<li>Unsupervised Learning</li>
<li>Automated Machine Learning</li>
</ul>
</li>
<li>Knowledge and Reasoning <ul>
<li>Representing and Inference with logic</li>
<li>Representing and Inference with Uncertainty </li>
<li>Knowledge Graph andRecommender System </li>
</ul>
</li>
</ul>
<h2 id="Connection-with-Previous-Courses"><a href="#Connection-with-Previous-Courses" class="headerlink" title="Connection with Previous Courses"></a>Connection with Previous Courses</h2><ul>
<li>In searching module, we use algorithm of graph, which we’ve learnt in <strong>DSAA</strong>.</li>
<li>In ML module, we use knowledge in <strong>Big Data</strong> Course.<ul>
<li>Also we talked about FOL … which is in <strong>Discrete Mathemetic</strong>.</li>
</ul>
</li>
<li>In KG-RS module, we use knowledge in <strong>Probability Theory and Mathemetic Statistics</strong>.</li>
<li>And the whole AI Course has strong connection to <strong>Calculus</strong> and <strong>Linear Algebra</strong>.</li>
</ul>
<blockquote>
<p>Therefore, if you want to learn AI well, these courses should be premises.</p>
<p>(Also said to me, a foolish student …)</p>
</blockquote>

    </div>
    
    <div>
      
      <div>
    
        <div style="text-align:center;color: #9d9d9d;font-size:18px;">------------- 本文结束 <i class="fas fa-book-reader"></i> 感谢阅读 -------------</div>
    
</div>
      
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/CSE-Learning/" rel="tag"># CSE Learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/09/13/Operating-Systems/" rel="prev" title="CS302 计算机操作系统">
                  <i class="fa fa-angle-left"></i> CS302 计算机操作系统
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/06/08/Management/" rel="next" title="EBA106 管理学">
                  EBA106 管理学 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  <script src="/js/third-party/pace.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","cdn":"//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
